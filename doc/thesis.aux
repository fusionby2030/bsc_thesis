\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{3}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Why Pedestal Physics?}{3}{subsection.1.1}\protected@file@percent }
\newlabel{subsec:purpose}{{1.1}{3}{Why Pedestal Physics?}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}JET Pedestal Database}{4}{subsection.1.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{subfig:plasmashaping}{{1a}{5}{\relax }{figure.caption.1}{}}
\newlabel{sub@subfig:plasmashaping}{{a}{5}{\relax }{figure.caption.1}{}}
\newlabel{subfig:ped_fit}{{1b}{5}{\relax }{figure.caption.1}{}}
\newlabel{sub@subfig:ped_fit}{{b}{5}{\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) shaping figure showing r,a, triangularity and such (b) Pedestal Values before mtanh fit\relax }}{5}{figure.caption.1}\protected@file@percent }
\newlabel{fig:pedestal_db_figs}{{1}{5}{(a) shaping figure showing r,a, triangularity and such (b) Pedestal Values before mtanh fit\relax }{figure.caption.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Main engineering parameter domains of the filtered dataset.\relax }}{6}{table.caption.2}\protected@file@percent }
\newlabel{tab:main_domain}{{1}{6}{Main engineering parameter domains of the filtered dataset.\relax }{table.caption.2}{}}
\newlabel{eq:mtanh}{{1}{6}{JET Pedestal Database}{equation.1.1}{}}
\newlabel{subfig:rvsneped}{{2a}{7}{\relax }{figure.caption.3}{}}
\newlabel{sub@subfig:rvsneped}{{a}{7}{\relax }{figure.caption.3}{}}
\newlabel{subfig:corr_shit}{{2b}{7}{\relax }{figure.caption.3}{}}
\newlabel{sub@subfig:corr_shit}{{b}{7}{\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (a) the 'ice cream corerlation' between the major radius $R$, divertor configuration, and $n_e^{ped}$, (b) the correlation matrix of the main engineering parameters to be used as inputs. A grey coloring represents no correlation, whereas blue and yellow are negative and positive correlation respectively. \relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:emperical}{{2}{7}{(a) the 'ice cream corerlation' between the major radius $R$, divertor configuration, and $n_e^{ped}$, (b) the correlation matrix of the main engineering parameters to be used as inputs. A grey coloring represents no correlation, whereas blue and yellow are negative and positive correlation respectively. \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Empirical Analysis}{7}{subsubsection.1.2.1}\protected@file@percent }
\newlabel{eq:scaling}{{2}{7}{Empirical Analysis}{equation.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}General Machine Learning Analysis}{9}{section.2}\protected@file@percent }
\newlabel{sec:principle-machine-learning-analysis}{{2}{9}{General Machine Learning Analysis}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Model Fitting and Validation}{9}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Linear Regression}{10}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Gaussian Processes}{10}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Random Forests}{11}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Artificial Neural Networks}{12}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Meta-Modeling}{12}{subsection.2.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Uncertainty in performance is derived from the cross-validation method, where the final metric is the average of the performances across each fold, and the uncert. is the std of the performance acros seach fold.\relax }}{13}{table.caption.4}\protected@file@percent }
\newlabel{tab:performance_models}{{2}{13}{Uncertainty in performance is derived from the cross-validation method, where the final metric is the average of the performances across each fold, and the uncert. is the std of the performance acros seach fold.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{13}{section.3}\protected@file@percent }
\newlabel{sec:results_1}{{3}{13}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Linear Regression}{13}{subsection.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Coefficients determined by Bayesian Linear Regression. Each coefficent is a normal distribution with mean $\mu $ and spread $\sigma ^2$\relax }}{13}{table.caption.5}\protected@file@percent }
\newlabel{tab:new_coef}{{3}{13}{Coefficients determined by Bayesian Linear Regression. Each coefficent is a normal distribution with mean $\mu $ and spread $\sigma ^2$\relax }{table.caption.5}{}}
\newlabel{subfig:baysian_results}{{3a}{14}{\relax }{figure.caption.6}{}}
\newlabel{sub@subfig:baysian_results}{{a}{14}{\relax }{figure.caption.6}{}}
\newlabel{subfig:bayesian_uq}{{3b}{14}{\relax }{figure.caption.6}{}}
\newlabel{sub@subfig:bayesian_uq}{{b}{14}{\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (a) The predictions of a Bayesian Regressor (green) vs the scaling law (orange) (b) Prediction uncertainty for the entries of the Bayesian regressor(b) Prediction uncertainty for the entries of the Bayesian regressor(b) Prediction uncertainty for the entries of the Bayesian regressor\relax }}{14}{figure.caption.6}\protected@file@percent }
\newlabel{fig:lin_reg}{{3}{14}{(a) The predictions of a Bayesian Regressor (green) vs the scaling law (orange) (b) Prediction uncertainty for the entries of the Bayesian regressor(b) Prediction uncertainty for the entries of the Bayesian regressor(b) Prediction uncertainty for the entries of the Bayesian regressor\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Gaussian Process}{14}{subsection.3.2}\protected@file@percent }
\newlabel{subfig:ARD_KL_VAR}{{4a}{15}{\relax }{figure.caption.7}{}}
\newlabel{sub@subfig:ARD_KL_VAR}{{a}{15}{\relax }{figure.caption.7}{}}
\newlabel{subfig:GP_vs_dimensionality}{{4b}{15}{\relax }{figure.caption.7}{}}
\newlabel{sub@subfig:GP_vs_dimensionality}{{b}{15}{\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces (a) The result of sensitivity analysis as well as the average of the three types used plotted in the dashed black line. (b)The dimensionality order of input variables comes from their ranking via the average of the three sensitivity analyses (dashed black line in diagram to the left). For each kernel, a GP model is fit using cros-validation (5 folds, 5 repeats) for each additional dimension of data, starting with 1d input of $\delta $, followed by 2d input of $\delta , a$ and so on. Then the RMSE for each repeated fold is calculated and averaged. \relax }}{15}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Three different methods UQ compared define homo, heteroscedastic, and fixed. The LHS, the residiual, and RHS, prediction uncertainty.\relax }}{16}{figure.caption.8}\protected@file@percent }
\newlabel{fig:MLP_UQ}{{5}{16}{Three different methods UQ compared define homo, heteroscedastic, and fixed. The LHS, the residiual, and RHS, prediction uncertainty.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Three different methods UQ compared. The LHS, the residiual, and RHS, prediction uncertainty.\relax }}{16}{figure.caption.9}\protected@file@percent }
\newlabel{fig:RQ_UQ}{{6}{16}{Three different methods UQ compared. The LHS, the residiual, and RHS, prediction uncertainty.\relax }{figure.caption.9}{}}
\newlabel{subfig:rfoob}{{7a}{17}{\relax }{figure.caption.10}{}}
\newlabel{sub@subfig:rfoob}{{a}{17}{\relax }{figure.caption.10}{}}
\newlabel{subfig:rf_rmse}{{7b}{17}{\relax }{figure.caption.10}{}}
\newlabel{sub@subfig:rf_rmse}{{b}{17}{\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces OOB error vs number of decision trees of (a) Random Forests (b) Extreme Random Trees\relax }}{17}{figure.caption.10}\protected@file@percent }
\newlabel{fig:RF_exp}{{7}{17}{OOB error vs number of decision trees of (a) Random Forests (b) Extreme Random Trees\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Random Forests and Ensemble Methods}{17}{subsection.3.3}\protected@file@percent }
\newlabel{subfig:RF_UQ}{{8a}{18}{\relax }{figure.caption.11}{}}
\newlabel{sub@subfig:RF_UQ}{{a}{18}{\relax }{figure.caption.11}{}}
\newlabel{subfig:RF_UQ2}{{8b}{18}{\relax }{figure.caption.11}{}}
\newlabel{sub@subfig:RF_UQ2}{{b}{18}{\relax }{figure.caption.11}{}}
\newlabel{subfig:RF_meta_model}{{8c}{18}{\relax }{figure.caption.11}{}}
\newlabel{sub@subfig:RF_meta_model}{{c}{18}{\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \relax }}{18}{figure.caption.11}\protected@file@percent }
\newlabel{fig:RF_Preds}{{8}{18}{\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Artificial Neural Networks}{18}{subsection.3.4}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{biblioski}
\newlabel{subfig:ann_1}{{9a}{19}{\relax }{figure.caption.12}{}}
\newlabel{sub@subfig:ann_1}{{a}{19}{\relax }{figure.caption.12}{}}
\newlabel{subfig:ensemble}{{9b}{19}{\relax }{figure.caption.12}{}}
\newlabel{sub@subfig:ensemble}{{b}{19}{\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}What didn't work}{19}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion and Outlook}{19}{section.4}\protected@file@percent }
