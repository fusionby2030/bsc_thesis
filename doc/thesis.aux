\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{EUROfusionroadmap}
\citation{Ikeda_2007}
\citation{stepladder}
\citation{Meneghini_2017}
\citation{MOREAU2011535}
\citation{Lawson_1957}
\citation{JT60_triple}
\citation{EUROfusionroadmap}
\citation{zohm_use_2019}
\citation{Troyon_1988}
\citation{FEDERICI2014882}
\citation{Ikeda_2007}
\citation{Ikeda_2007}
\citation{Ikeda_2007}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{3}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Why Pedestal Physics?}{3}{subsection.1.1}\protected@file@percent }
\newlabel{subsec:purpose}{{1.1}{3}{Why Pedestal Physics?}{subsection.1.1}{}}
\citation{PhysRevLett}
\citation{Martin_2008}
\citation{ELM_s}
\citation{Viezzer_2018}
\citation{ELM_s}
\citation{PHILIPPS20101581}
\citation{Frassinetti_2020}
\citation{electron_transport}
\citation{Catto_2013}
\citation{Kotschenreuther_2019}
\citation{EPED_ELM}
\citation{Snyder_2011}
\citation{Frassinetti_2020}
\citation{Saarelma_2017}
\citation{pedestal_prediction}
\citation{Frassinetti_2020}
\citation{Pasqualotto_2004}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}JET Pedestal Database}{4}{subsection.1.2}\protected@file@percent }
\newlabel{eq:mtanh}{{1}{4}{JET Pedestal Database}{equation.1.1}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{subfig:triangularity}{{1a}{5}{\relax }{figure.caption.1}{}}
\newlabel{sub@subfig:triangularity}{{a}{5}{\relax }{figure.caption.1}{}}
\newlabel{subfig:mtanhfit}{{1b}{5}{\relax }{figure.caption.1}{}}
\newlabel{sub@subfig:mtanhfit}{{b}{5}{\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \textbf  {(a)} Plasma shape parameters from the cross-section of a tokamak. \textbf  {(b)} HRTS measurement profiles (blue) radially shifted to have $T_e \approx 100$ eV at the seperatrix (LCFS). The profiles are fitted in real space using the mtanh equation (eq. \ref  {eq:mtanh}) then mapped to the normalized poloidal flux coordinate $\Psi _N$ (red).\relax }}{5}{figure.caption.1}\protected@file@percent }
\newlabel{fig:pedestal_db_figs}{{1}{5}{\textbf {(a)} Plasma shape parameters from the cross-section of a tokamak. \textbf {(b)} HRTS measurement profiles (blue) radially shifted to have $T_e \approx 100$ eV at the seperatrix (LCFS). The profiles are fitted in real space using the mtanh equation (eq. \ref {eq:mtanh}) then mapped to the normalized poloidal flux coordinate $\Psi _N$ (red).\relax }{figure.caption.1}{}}
\citation{Frassinetti_2020}
\citation{Frassinetti_2020}
\citation{Frassinetti_2020}
\citation{Viezzer_2018}
\citation{Frassinetti_2020}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Main engineering parameter domains of the filtered dataset.\relax }}{6}{table.caption.2}\protected@file@percent }
\newlabel{tab:main_domain}{{1}{6}{Main engineering parameter domains of the filtered dataset.\relax }{table.caption.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Domains of pedestal parameters for deuterium shots stored in the JET pedestal database after RMPs, kicks, pellets, and non-validated HRTS are filtered out.\relax }}{6}{table.caption.3}\protected@file@percent }
\newlabel{tab:ped_quant}{{2}{6}{Domains of pedestal parameters for deuterium shots stored in the JET pedestal database after RMPs, kicks, pellets, and non-validated HRTS are filtered out.\relax }{table.caption.3}{}}
\citation{Frassinetti_2020}
\citation{shafranov_equilibrium_1963}
\citation{freidberg_plasma_2007}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Empirical Analysis}{7}{subsubsection.1.2.1}\protected@file@percent }
\newlabel{eq:scaling}{{2}{7}{Empirical Analysis}{equation.1.2}{}}
\newlabel{subfig:icecream}{{2a}{7}{\relax }{figure.caption.4}{}}
\newlabel{sub@subfig:icecream}{{a}{7}{\relax }{figure.caption.4}{}}
\newlabel{subfig:corr}{{2b}{7}{\relax }{figure.caption.4}{}}
\newlabel{sub@subfig:corr}{{b}{7}{\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Empirical data plots of the JET pedestal database: \textbf  {(a)} correlation without causation between the major radius $R$ and $n_e^{ped}$, \textbf  {(b)} Correlation matrix of the main engineering parameters. A grey coloring represents no correlation, whereas blue and yellow are negative and positive correlation respectively.\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:emperical}{{2}{7}{Empirical data plots of the JET pedestal database: \textbf {(a)} correlation without causation between the major radius $R$ and $n_e^{ped}$, \textbf {(b)} Correlation matrix of the main engineering parameters. A grey coloring represents no correlation, whereas blue and yellow are negative and positive correlation respectively.\relax }{figure.caption.4}{}}
\citation{OLS}
\citation{Selection_Bias}
\@writefile{toc}{\contentsline {section}{\numberline {2}General Machine Learning Analysis}{8}{section.2}\protected@file@percent }
\newlabel{sec:principle-machine-learning-analysis}{{2}{8}{General Machine Learning Analysis}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Model Fitting and Validation}{8}{subsection.2.1}\protected@file@percent }
\citation{Frassinetti_2020}
\citation{hastie01statisticallearning}
\citation{Bisong2019}
\citation{LASSO_OG}
\citation{LASSO_COIN}
\citation{bayes_regr}
\citation{scikit-learn}
\citation{Salvatier2016}
\citation{gortler2019a}
\citation{Rasmussen2004}
\citation{vapnik95}
\citation{cov-matrix-maths}
\citation{Rasmussen2004}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Linear Regression}{9}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Gaussian Processes}{9}{subsection.2.3}\protected@file@percent }
\citation{pmlr-v89-paananen19a}
\citation{heteroscedastic}
\citation{pmlr-v89-paananen19a}
\citation{KLD}
\citation{pmlr-v89-paananen19a}
\citation{pmlr-v89-paananen19a}
\citation{gpy2014}
\citation{DT_OG}
\citation{RF_OG}
\citation{598994}
\citation{hastie01statisticallearning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Random Forests}{10}{subsection.2.4}\protected@file@percent }
\citation{geurts_extremely_2006}
\citation{scikit-learn}
\citation{silver_mastering_2016}
\citation{7333916}
\citation{CALTECH}
\citation{Goodfellow-et-al-2016}
\citation{SCHMIDHUBER201585}
\citation{Andreas}
\citation{NEURIPS2019_9015}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Artificial Neural Networks}{11}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Meta-Modeling}{11}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{12}{section.3}\protected@file@percent }
\newlabel{sec:results_1}{{3}{12}{Results}{section.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The best RMSE and MAE of the predictions from each optimal model are calculated by averaging the results across each fold and repeat of the repeated cross-validation method. Uncertainty in the calculated RMSE and MAE is derived from the standard deviation of the RMSE across each fold.\relax }}{12}{table.caption.5}\protected@file@percent }
\newlabel{tab:performance_models}{{3}{12}{The best RMSE and MAE of the predictions from each optimal model are calculated by averaging the results across each fold and repeat of the repeated cross-validation method. Uncertainty in the calculated RMSE and MAE is derived from the standard deviation of the RMSE across each fold.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Linear Regression}{12}{subsection.3.1}\protected@file@percent }
\newlabel{subfig:bayes_pred}{{3a}{12}{\relax }{figure.caption.6}{}}
\newlabel{sub@subfig:bayes_pred}{{a}{12}{\relax }{figure.caption.6}{}}
\newlabel{subfig:bayes_resid}{{3b}{12}{\relax }{figure.caption.6}{}}
\newlabel{sub@subfig:bayes_resid}{{b}{12}{\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of a Bayesian Ridge regressor fit using all available engineering parameters against the log-linear scaling law eq.\tmspace  +\thickmuskip {.2777em}( \ref  {eq:scaling}). \textbf  {(a)} The predictions of a Bayesian regressor (green) vs the scaling law (orange) with the ground truth being the black dotted line. \textbf  {(b)} The Residual comparison of the Bayesian regressor (green) and the scaling law (orange) \relax }}{12}{figure.caption.6}\protected@file@percent }
\newlabel{fig:lin_reg}{{3}{12}{Comparison of a Bayesian Ridge regressor fit using all available engineering parameters against the log-linear scaling law eq.\;( \ref {eq:scaling}). \textbf {(a)} The predictions of a Bayesian regressor (green) vs the scaling law (orange) with the ground truth being the black dotted line. \textbf {(b)} The Residual comparison of the Bayesian regressor (green) and the scaling law (orange) \relax }{figure.caption.6}{}}
\citation{kernel_cookbook}
\citation{gpy_MLP}
\citation{kernel_cookbook}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Coefficients determined by Bayesian Linear Regression. Each coefficient is a normal distribution with mean $\mu $ and spread $\sigma ^2$\relax }}{13}{table.caption.7}\protected@file@percent }
\newlabel{tab:new_coef}{{4}{13}{Coefficients determined by Bayesian Linear Regression. Each coefficient is a normal distribution with mean $\mu $ and spread $\sigma ^2$\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Gaussian Process}{13}{subsection.3.2}\protected@file@percent }
\newlabel{subfig:GP_sens}{{4a}{14}{\relax }{figure.caption.8}{}}
\newlabel{sub@subfig:GP_sens}{{a}{14}{\relax }{figure.caption.8}{}}
\newlabel{subfig:GP_dimens}{{4b}{14}{\relax }{figure.caption.8}{}}
\newlabel{sub@subfig:GP_dimens}{{b}{14}{\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Steps towards dimensionality reduction through the use of Gaussian Processes. \textbf  {(a)} The result of sensitivity analysis as well as the average of the three types used plotted in the dashed black line. \textbf  {(b)} The dimensionality order of input variables comes from their ranking via the average of the three sensitivity analyses (dashed black line in diagram to the left). For each kernel, a GP model is fit using cross-validation (5 folds, 5 repeats) for each additional dimension of data, starting with 1d input of $\delta $, followed by 2D input of $\delta , a$ and so on. Then the RMSE is calculated on folds left out and averaged across all folds. \relax }}{14}{figure.caption.8}\protected@file@percent }
\newlabel{fig:GP_dim}{{4}{14}{Steps towards dimensionality reduction through the use of Gaussian Processes. \textbf {(a)} The result of sensitivity analysis as well as the average of the three types used plotted in the dashed black line. \textbf {(b)} The dimensionality order of input variables comes from their ranking via the average of the three sensitivity analyses (dashed black line in diagram to the left). For each kernel, a GP model is fit using cross-validation (5 folds, 5 repeats) for each additional dimension of data, starting with 1d input of $\delta $, followed by 2D input of $\delta , a$ and so on. Then the RMSE is calculated on folds left out and averaged across all folds. \relax }{figure.caption.8}{}}
\newlabel{subfig:MLPW_RES}{{5a}{14}{\relax }{figure.caption.9}{}}
\newlabel{sub@subfig:MLPW_RES}{{a}{14}{\relax }{figure.caption.9}{}}
\newlabel{subfig:MLP_UQ}{{5b}{14}{\relax }{figure.caption.9}{}}
\newlabel{sub@subfig:MLP_UQ}{{b}{14}{\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Three different methods of uncertainty quantification are compared for a GP with an MLP kernel. \textbf  {(a)} The true values of $n_e^{ped}$ are grouped into 11 equal-sized bins and the distance between the predictions of those values and the true values are calculated. The residuals are averaged across each bin and compared between the methods. \textbf  {(b)} The same binning procedure, but with averaging the prediction uncertainties of the same three methods are compared.\relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig:MLP_UQ}{{5}{14}{Three different methods of uncertainty quantification are compared for a GP with an MLP kernel. \textbf {(a)} The true values of $n_e^{ped}$ are grouped into 11 equal-sized bins and the distance between the predictions of those values and the true values are calculated. The residuals are averaged across each bin and compared between the methods. \textbf {(b)} The same binning procedure, but with averaging the prediction uncertainties of the same three methods are compared.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Random Forests}{15}{subsection.3.3}\protected@file@percent }
\newlabel{subfig:RF_oob}{{6a}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@subfig:RF_oob}{{a}{15}{\relax }{figure.caption.10}{}}
\newlabel{subfig:ERT_oob}{{6b}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@subfig:ERT_oob}{{b}{15}{\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces OOB error vs number of decision trees of \textbf  {(a)} Random Forests \textbf  {(b)} Extreme Random Trees. No cross validation is necessary for this procedure, since the process of bagging inherently prevents all trees from seeing the same data. The colored lines correspond to different RFs and ERTs which vary by the amount of features to sample when splitting a node.\relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{fig:RF_exp}{{6}{15}{OOB error vs number of decision trees of \textbf {(a)} Random Forests \textbf {(b)} Extreme Random Trees. No cross validation is necessary for this procedure, since the process of bagging inherently prevents all trees from seeing the same data. The colored lines correspond to different RFs and ERTs which vary by the amount of features to sample when splitting a node.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Effect of meat-modeling in RF and ERTs. The \# of synthesized samples added into dataset is plotted against the resulting RMSE of a model fitted using the synthesized samples.\relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{fig:RF_meta_model}{{7}{16}{Effect of meat-modeling in RF and ERTs. The \# of synthesized samples added into dataset is plotted against the resulting RMSE of a model fitted using the synthesized samples.\relax }{figure.caption.11}{}}
\newlabel{subfig:RF_RES}{{8a}{16}{\relax }{figure.caption.12}{}}
\newlabel{sub@subfig:RF_RES}{{a}{16}{\relax }{figure.caption.12}{}}
\newlabel{subfig:RF_UQ}{{8b}{16}{\relax }{figure.caption.12}{}}
\newlabel{sub@subfig:RF_UQ}{{b}{16}{\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of predictions and uncertainties between Random Forests (green) and Extreme Random Trees (blue). \textbf  {(a)} The residual between predictions and the true values,\textbf  {(b)} The prediction uncertainties on the same bins of the residuals. \relax }}{16}{figure.caption.12}\protected@file@percent }
\newlabel{fig:RF_Preds}{{8}{16}{Comparison of predictions and uncertainties between Random Forests (green) and Extreme Random Trees (blue). \textbf {(a)} The residual between predictions and the true values,\textbf {(b)} The prediction uncertainties on the same bins of the residuals. \relax }{figure.caption.12}{}}
\citation{pytorch_act}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Artificial Neural Networks}{17}{subsection.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Comparison of the size of an ANN and its predictive capability. Each point represents an ANN which hidden layers are equal in size.\relax }}{17}{figure.caption.13}\protected@file@percent }
\newlabel{fig:ANN_dim}{{9}{17}{Comparison of the size of an ANN and its predictive capability. Each point represents an ANN which hidden layers are equal in size.\relax }{figure.caption.13}{}}
\citation{Mucherino2009}
\citation{hastie01statisticallearning}
\citation{schapire2013explaining}
\newlabel{subfig:ANN_res}{{10a}{18}{\relax }{figure.caption.14}{}}
\newlabel{sub@subfig:ANN_res}{{a}{18}{\relax }{figure.caption.14}{}}
\newlabel{subfig:ANN_uncert}{{10b}{18}{\relax }{figure.caption.14}{}}
\newlabel{sub@subfig:ANN_uncert}{{b}{18}{\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A three layer feed forward fully connected ANN with layer sizes 483, 415, 254 was transformed into ensembles of sizes 5 (blue), 10 (orange, 15 (green) and compared: \textbf  {(a)} The residual between predictions and the true values, \textbf  {(b)} The prediction uncertainties on the same bins of the residuals. \relax }}{18}{figure.caption.14}\protected@file@percent }
\newlabel{fig:ANN_UQ}{{10}{18}{A three layer feed forward fully connected ANN with layer sizes 483, 415, 254 was transformed into ensembles of sizes 5 (blue), 10 (orange, 15 (green) and compared: \textbf {(a)} The residual between predictions and the true values, \textbf {(b)} The prediction uncertainties on the same bins of the residuals. \relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}What did not work}{18}{subsection.3.5}\protected@file@percent }
\citation{WU2018417}
\bibstyle{unsrt}
\bibdata{biblioski}
\bibcite{EUROfusionroadmap}{1}
\bibcite{Ikeda_2007}{2}
\bibcite{stepladder}{3}
\bibcite{Meneghini_2017}{4}
\bibcite{MOREAU2011535}{5}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion and Outlook}{19}{section.4}\protected@file@percent }
\bibcite{Lawson_1957}{6}
\bibcite{JT60_triple}{7}
\bibcite{zohm_use_2019}{8}
\bibcite{Troyon_1988}{9}
\bibcite{FEDERICI2014882}{10}
\bibcite{PhysRevLett}{11}
\bibcite{Martin_2008}{12}
\bibcite{ELM_s}{13}
\bibcite{Viezzer_2018}{14}
\bibcite{PHILIPPS20101581}{15}
\bibcite{Frassinetti_2020}{16}
\bibcite{electron_transport}{17}
\bibcite{Catto_2013}{18}
\bibcite{Kotschenreuther_2019}{19}
\bibcite{EPED_ELM}{20}
\bibcite{Snyder_2011}{21}
\bibcite{Saarelma_2017}{22}
\bibcite{pedestal_prediction}{23}
\bibcite{Pasqualotto_2004}{24}
\bibcite{shafranov_equilibrium_1963}{25}
\bibcite{freidberg_plasma_2007}{26}
\bibcite{OLS}{27}
\bibcite{Selection_Bias}{28}
\bibcite{hastie01statisticallearning}{29}
\bibcite{Bisong2019}{30}
\bibcite{LASSO_OG}{31}
\bibcite{LASSO_COIN}{32}
\bibcite{bayes_regr}{33}
\bibcite{scikit-learn}{34}
\bibcite{Salvatier2016}{35}
\bibcite{gortler2019a}{36}
\bibcite{Rasmussen2004}{37}
\bibcite{vapnik95}{38}
\bibcite{cov-matrix-maths}{39}
\bibcite{pmlr-v89-paananen19a}{40}
\bibcite{heteroscedastic}{41}
\bibcite{KLD}{42}
\bibcite{gpy2014}{43}
\bibcite{DT_OG}{44}
\bibcite{RF_OG}{45}
\bibcite{598994}{46}
\bibcite{geurts_extremely_2006}{47}
\bibcite{silver_mastering_2016}{48}
\bibcite{7333916}{49}
\bibcite{CALTECH}{50}
\bibcite{Goodfellow-et-al-2016}{51}
\bibcite{SCHMIDHUBER201585}{52}
\bibcite{Andreas}{53}
\bibcite{NEURIPS2019_9015}{54}
\bibcite{kernel_cookbook}{55}
\bibcite{gpy_MLP}{56}
\bibcite{pytorch_act}{57}
\bibcite{Mucherino2009}{58}
\bibcite{schapire2013explaining}{59}
\bibcite{WU2018417}{60}
