\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{3}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Why Pedestal Physics?}{3}{subsection.1.1}\protected@file@percent }
\newlabel{subsec:purpose}{{1.1}{3}{Why Pedestal Physics?}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}JET Pedestal Database}{4}{subsection.1.2}\protected@file@percent }
\newlabel{eq:mtanh}{{1}{4}{JET Pedestal Database}{equation.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \textbf  {Left:} A shaping figure showing r,a, triangularity. Resembles a bow \textbf  {Right:} HRTS measurement profiles (blue) radially shifted to have $T_e \approx 100$ eV at the seperatrix (LCFS). The profiles are fitted in real space using the mtanh equation \ref  {eq:mtanh} then mapped to the normalized poloidal flux coordinate $\Psi _N$ (red).\relax }}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pedestal_db_figs}{{1}{5}{\textbf {Left:} A shaping figure showing r,a, triangularity. Resembles a bow \textbf {Right:} HRTS measurement profiles (blue) radially shifted to have $T_e \approx 100$ eV at the seperatrix (LCFS). The profiles are fitted in real space using the mtanh equation \ref {eq:mtanh} then mapped to the normalized poloidal flux coordinate $\Psi _N$ (red).\relax }{figure.caption.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Main engineering parameter domains of the filtered dataset.\relax }}{6}{table.caption.2}\protected@file@percent }
\newlabel{tab:main_domain}{{1}{6}{Main engineering parameter domains of the filtered dataset.\relax }{table.caption.2}{}}
\newlabel{tab:ped_quant}{{\caption@xref {tab:ped_quant}{ on input line 231}}{7}{JET Pedestal Database}{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Domains of pedestal paramaters for deuterium shots stored in the JET pedestal database after RMPs, Kicks, Pellets are filtered out.\relax }}{7}{table.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Emperical data plots of the JET pedestal database. \textbf  {Left:} the 'ice cream corerlation' between the major radius $R$, divertor configuration, and $n_e^{ped}$, \textbf  {Right:} correlation matrix of the main engineering parameters. A grey coloring represents no correlation, whereas blue and yellow are negative and positive correlation respectively.\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:emperical}{{2}{7}{Emperical data plots of the JET pedestal database. \textbf {Left:} the 'ice cream corerlation' between the major radius $R$, divertor configuration, and $n_e^{ped}$, \textbf {Right:} correlation matrix of the main engineering parameters. A grey coloring represents no correlation, whereas blue and yellow are negative and positive correlation respectively.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Empirical Analysis}{7}{subsubsection.1.2.1}\protected@file@percent }
\newlabel{eq:scaling}{{2}{7}{Empirical Analysis}{equation.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}General Machine Learning Analysis}{9}{section.2}\protected@file@percent }
\newlabel{sec:principle-machine-learning-analysis}{{2}{9}{General Machine Learning Analysis}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Model Fitting and Validation}{9}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Linear Regression}{9}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Gaussian Processes}{10}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Random Forests}{11}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Artificial Neural Networks}{12}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Meta-Modeling}{12}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{13}{section.3}\protected@file@percent }
\newlabel{sec:results_1}{{3}{13}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Linear Regression}{13}{subsection.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Coefficients determined by Bayesian Linear Regression. Each coefficent is a normal distribution with mean $\mu $ and spread $\sigma ^2$\relax }}{13}{table.caption.5}\protected@file@percent }
\newlabel{tab:new_coef}{{3}{13}{Coefficients determined by Bayesian Linear Regression. Each coefficent is a normal distribution with mean $\mu $ and spread $\sigma ^2$\relax }{table.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of a Bayesian Ridge Regressor fit using all available engineering parameters against the log-linear scaling law give in \ref  {eq:scaling}. \textbf  {Left:} The predictions of a Bayesian Regressor (green) vs the scaling law (orange) with the ground truth being the black dotted line. \textbf  {Right:} The Residual comparison of the bayesian regressor (green) and the scaling law (orange) \relax }}{14}{figure.caption.6}\protected@file@percent }
\newlabel{fig:lin_reg}{{3}{14}{Comparison of a Bayesian Ridge Regressor fit using all available engineering parameters against the log-linear scaling law give in \ref {eq:scaling}. \textbf {Left:} The predictions of a Bayesian Regressor (green) vs the scaling law (orange) with the ground truth being the black dotted line. \textbf {Right:} The Residual comparison of the bayesian regressor (green) and the scaling law (orange) \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Gaussian Process}{14}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Steps towards dimensionality reduction through the use of Gaussian Processes. \textbf  {Left:} The result of sensitivity analysis as well as the average of the three types used plotted in the dashed black line. \textbf  {Right:} The dimensionality order of input variables comes from their ranking via the average of the three sensitivity analyses (dashed black line in diagram to the left). For each kernel, a GP model is fit using cross-validation (5 folds, 5 repeats) for each additional dimension of data, starting with 1d input of $\delta $, followed by 2d input of $\delta , a$ and so on. Then the RMSE is calculated on folds left out and averaged across all folds. \relax }}{15}{figure.caption.7}\protected@file@percent }
\newlabel{fig:GP_dim}{{4}{15}{Steps towards dimensionality reduction through the use of Gaussian Processes. \textbf {Left:} The result of sensitivity analysis as well as the average of the three types used plotted in the dashed black line. \textbf {Right:} The dimensionality order of input variables comes from their ranking via the average of the three sensitivity analyses (dashed black line in diagram to the left). For each kernel, a GP model is fit using cross-validation (5 folds, 5 repeats) for each additional dimension of data, starting with 1d input of $\delta $, followed by 2d input of $\delta , a$ and so on. Then the RMSE is calculated on folds left out and averaged across all folds. \relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Three different methods of uncertainty quantification are compared for a GP with an MLP kernel. \textbf  {Left:} The true values of $n_e^{ped}$ are grouped into 11 equaly sized bins and the distance between the predictions of those values and the true values are caluclated. The residuals are averaged across each bin and compared between the methods. \textbf  {Right:} The same binning procedure, but with averaging the prediction uncertainties of the same three methods are compared.\relax }}{15}{figure.caption.8}\protected@file@percent }
\newlabel{fig:MLP_UQ}{{5}{15}{Three different methods of uncertainty quantification are compared for a GP with an MLP kernel. \textbf {Left:} The true values of $n_e^{ped}$ are grouped into 11 equaly sized bins and the distance between the predictions of those values and the true values are caluclated. The residuals are averaged across each bin and compared between the methods. \textbf {Right:} The same binning procedure, but with averaging the prediction uncertainties of the same three methods are compared.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Random Forests}{16}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces OOB error vs number of decision trees of \textbf  {Left:} Random Forests \textbf  {Right:} Extreme Random Trees. No cross validation is necessary for this procedure, since the process of bagging inherently prevents all trees from seeing the same data. The colored lines correspond to different RFs and ERTs which vary by the amount of features to sample when splitting a node.\relax }}{16}{figure.caption.9}\protected@file@percent }
\newlabel{fig:RF_exp}{{6}{16}{OOB error vs number of decision trees of \textbf {Left:} Random Forests \textbf {Right:} Extreme Random Trees. No cross validation is necessary for this procedure, since the process of bagging inherently prevents all trees from seeing the same data. The colored lines correspond to different RFs and ERTs which vary by the amount of features to sample when splitting a node.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of predictions and uncertainties between Random Forests (green) and Extreme Random Trees (blue). \textbf  {Left:} The residual between predictions and the true values,\textbf  {Right:} The prediction uncertainties on the same bins of the residuals. \relax }}{17}{figure.caption.11}\protected@file@percent }
\newlabel{fig:RF_Preds}{{8}{17}{Comparison of predictions and uncertainties between Random Forests (green) and Extreme Random Trees (blue). \textbf {Left:} The residual between predictions and the true values,\textbf {Right:} The prediction uncertainties on the same bins of the residuals. \relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Effect of Meta Modeling in RF and ERTs. The \# of synthesized samples added into dataset is plotted against the resulting RMSE of an RF/ERT that is fit with the additional number of synthesized samples added into its training dataset.\relax }}{17}{figure.caption.10}\protected@file@percent }
\newlabel{fig:RF_meta_model}{{7}{17}{Effect of Meta Modeling in RF and ERTs. The \# of synthesized samples added into dataset is plotted against the resulting RMSE of an RF/ERT that is fit with the additional number of synthesized samples added into its training dataset.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Artificial Neural Networks}{18}{subsection.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Comparison of the size of an ANN and its predictive capability. Each point represents an ANN which hidden layers are equal in size.\relax }}{18}{figure.caption.12}\protected@file@percent }
\newlabel{fig:ANN_dim}{{9}{18}{Comparison of the size of an ANN and its predictive capability. Each point represents an ANN which hidden layers are equal in size.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A three layer feed foward fully connected ANN with layer sizes 483, 415, 254 was transformed into ensembles of sizes 5 (blue), 10 (orange, 15 (green). \relax }}{19}{figure.caption.13}\protected@file@percent }
\newlabel{fig:ANN_UQ}{{10}{19}{A three layer feed foward fully connected ANN with layer sizes 483, 415, 254 was transformed into ensembles of sizes 5 (blue), 10 (orange, 15 (green). \relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}What did not work}{19}{subsection.3.5}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{biblioski}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion and Outlook}{21}{section.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The optimal relevant hyperparameters are chose for each model type, and the best RMSE and MAE are caulculated by averaging the results across each fold and repeat of the repeated cross-validation method. Uncertainty in the calculated RMSE and MAE is derived from the standard deviation of the RMSE across each fold.\relax }}{21}{table.caption.14}\protected@file@percent }
\newlabel{tab:performance_models}{{4}{21}{The optimal relevant hyperparameters are chose for each model type, and the best RMSE and MAE are caulculated by averaging the results across each fold and repeat of the repeated cross-validation method. Uncertainty in the calculated RMSE and MAE is derived from the standard deviation of the RMSE across each fold.\relax }{table.caption.14}{}}
