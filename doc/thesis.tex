
%! Author = adam
%! Date = 16.04.21

% Preamble
\documentclass[a4paper, twoside, final, 12pt]{article}

% Packages
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[a4paper, total={170mm,257mm}, left=20mm, top=20mm]{geometry}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{floatrow}
\usepackage{adjustbox}
\usepackage{wrapfig}
\graphicspath{{./src/ }}
\usepackage{caption}
\usepackage{float}
\floatstyle{plaintop}
\restylefloat{table}
\usepackage{tabularx}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\newcommand\setrow[1]{\gdef\rowmac{#1}#1\ignorespaces}
\newcommand\clearrow{\global\let\rowmac\relax}
\newenvironment{packed_enum}{
\begin{itemize}
  \setlength{\itemsep}{5pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

% Page Setups

% Document


\title{
	{Machine Learning Analysis on the JET Pedestal Database } \vspace{0.5cm}
	{\large Univerisitat Leipzig} \\ 
	}

\author{Adam Kit \\{\small Advisors: Cichos, F. and Groth. M and Järvinen A.}}
\date{15 06 2021}
\clearrow
\begin{document}
\begin{titlepage}
   \begin{center}
       \vspace*{1cm}

       \textbf{Machine Learning Analysis on the JET Pedestal Database }

       \vspace{1.5cm}

       \textbf{Adam Kit}
       
       \vspace{0.5cm}
       \textbf{Advisors:} Cichos, F. and Groth, M and Järvinen, A.

       \vfill
            
       A thesis presented for the degree of\\
       B.Sc. Physics
            
       \vspace{0.8cm}
     
%       \includegraphics[width=0.4\textwidth]{university}
            
       Universität Leipzig\\
       Germany\\
       17.06.21
            
   \end{center}
\end{titlepage}
  %\maketitle
    \newpage
    \tableofcontents
    \newpage
\section{Introduction}\label{sec:introduction}
Harnessing controlled thermonuclear fusion on Earth is a complex multi-faceted problem; a potential solution is that of controlled magnetic confinement fusion (MCF), such as stellarators or tokamaks \cite{EUROfusionroadmap}. 
The field of MCF research is entering the era of superconducting, reactor-scale, long pulse devices, such as ITER and DEMO \cite{Ikeda_2007, stepladder}.
These reactor-scale devices encompass a significant risk of very costly component damages in off-normal events, and, hence, the emphasis on reactor and plasma scenario design is shifting from experimental approaches to theory based predict-first and plasma flight simulator methods \cite{Meneghini_2017, MOREAU2011535}.
To  bridge  the  gap  between computational and experimental efforts, and to rapidly design reactors, there exists a need for data-driven approaches to produce models through the use of machine learning (ML). 
\begin{comment}
To allow fast throughput for flight simulation and on the fly scenario modelling, there exists a need for data-driven approaches to produce models through the use of machine learning (ML).
\end{comment}
The topic of this thesis is to analyze and compare predictive ML tools in estimating plasma parameters based on experimental data. The focus is on the plasma density in the edge of tokamaks, where high performance scenarios typically establish an edge transport barrier and plasma pedestal. 

\subsection{Pedestal Physics}\label{subsec:purpose}
In tokamaks, the fusion plasma is confined in a toroidal vacuum chamber using magnetic fields.
The magnetic field has components around the torus, called the toroidal component, and around the cross-section of the vacuum chamber (Fig. \ref{subfig:triangularity}), called the poloidal component.
These are generated with magnetic coils and plasma currents.
The toroidal and poloidal magnetic fields generate helical field lines that are necessary to confine the plasma.
The helical field lines form nested closed flux surfaces.
At the edge of the plasma, structures of the reactor wall intersect these flux surfaces, such that they become open.
The region of the open field lines, within which the plasma is in contact with the reactor components, is called the scrape-off layer (SOL).
The last flux surface that is closed is aptly named the last closed flux surface (LCFS), and the field line that separates LCFS from SOL is called the seperatrix.
In present-day plasma scenarios, the edge plasma is magnetically diverted to a separate divertor area, such that the LCFS is not in direct contact with the wall.

Nuclear fusion with net energy gain requires sufficiently high fuel pressure and confinement time, i.e., the triple product of the density, temperature and confinement time, $nT\tau_E$,  must be high enough \cite{Lawson_1957}. A deuterium-tritium plasma is considered ignited when the heating provided by the resulting 3.5 MeV alpha particles is sufficient to overcome the energy losses. 
The highest achieved triple product in MCF devices thus far is $1.53\times 10^{21}\text{ keVs/m}^3$ in the JT-60 U tokamak operating with deuterium plasmas \cite{JT60_triple}. ITER and DEMO plan to achieve triple product values on the order of $10^{22}\text{ keVs/m}^3$ operating with deuterium-tritium plasmas \cite{EUROfusionroadmap}.

The maximum pressures achievable in MCF devices are limited by magnetic field strength and magnetohydrodynamic (MHD) instabilities.
The field strength is limited by the capabilities of the available superconductor technology such that, in conventional tokamak reactor designs, the fields in the center of the plasmas are around 5-6 T \cite{zohm_use_2019}. A useful figure of merit for an MCF device is the ratio of the confined fuel pressure to that of the magnetic field pressure, $\beta = p / (B/2\mu_0)$, which in tokamaks is always significantly less than unity for stable plasmas. Higher $\beta$ plasmas are desired for high fusion performance and power density, but the maximum $\beta$ is limited by MHD stability. Often $\beta$ is further normalized with the Troyon factor $(B_T \, a/ I_P)$, originating from the Troyon $\beta$ limit \cite{Troyon_1988}. This is called $\beta_N$. Typically in tokamaks, the outcome of these limits is that the confined fuel pressure is a few times the atmospheric pressure, and the energy confinement time is around seconds~\cite{FEDERICI2014882}.


The energy confinement time is limited by plasma turbulence, which leads to radial transport across the flux surfaces significantly faster than would be expected based on classical or neo-classical transport \cite{Ikeda_2007}.
Typically, the turbulence modes show critical gradient behavior \cite{Ikeda_2007}, such that the radial gradients are limited near their critical value, and the effective transport increases with more heating power.
This enhanced transport results in reduction of $\tau_E$ with heating power, as can be seen in common confinement time scalings, for low (L-mode) and high confinement mode (H-mode) \cite{Ikeda_2007}. As a result, reaching $nT\tau_E$ is challenging considering the solution of throwing power at the problem has the opposite than desired effect.

In the 1980s, a sudden transition into an enhanced confinement regime, H-mode, was discovered in plasmas operating with the divertor configurations and neutral beam heating \cite{PhysRevLett}.
The H-mode confinement can break away from the stiff gradients at the edge, as self-organized shear flows at the plasma edge reduce turbulent radial fluxes, leading to the formation of a plasma pressure 'pedestal'.
To achieve H-mode confinement, a minimum amount of power flow through the edge is required \cite{Martin_2008}. H-mode ultimately provides about a factor of two increase of the energy confinement time. Therefore, H-mode is the standard operational mode expected in future reactor-scale devices such as ITER and DEMO.

The suppressed turbulence in the pedestal region allows the radial pressure  and current gradients to grow until they trigger MHD instabilities, called edge localized modes (ELMs) \cite{ELM_s, Viezzer_2018}.
The current understanding is that high performance pedestal plasmas are limited by ideal MHD peeling-ballooning instabilities that trigger type-I ELMs \cite{ELM_s}.
Pedestal plasmas are not always limited by ideal MHD peeling-ballooning instabilities.
For example, a large fraction of JET H-mode plasmas operating with the ITER-like wall, including beryllium main chamber and tungsten divertor targets~\cite{PHILIPPS20101581}, do not reach the ideal MHD peeling-ballooning stability threshold \cite{Frassinetti_2020}.  Therefore, determining which transport or instability phenomena limit the pedestal is a very active topic of research~\cite{electron_transport, Catto_2013, Kotschenreuther_2019}.

Since edge transport barriers are key ingredients of future ITER and DEMO burning plasma scenarios, predictive capability is necessary for the pedestal region to confidently design the future reactors and scenarios.
Due to complexity of the pedestal plasmas and the non-linear interaction of the pedestal with the core and SOL plasmas, the simulation codes for the pedestal make simplifying assumptions. The most widely used model for pedestal pressure predictions is EPED \cite{EPED_ELM, Snyder_2011}. 
EPED has been very successful for predicting pedestal pressure width and height for many present-day tokamaks, by assuming that the pressure gradient in the pedestal is limited by local kinetic balooning modes (KBM) and the total pressure by ideal-MHD peeling-balooning modes. However, these assumptions are not justified for a large fraction of the JET pedestal database \cite{Frassinetti_2020}. EPED also takes as input certain plasma characteristics, including information about pedestal densities and density profiles, confined normalized pressure, and dilution of the plasmas by impurities. Therefore, EPED cannot be considered a fully predictive model. EUROPED aims to bridge some of these shortcomings of EPED by using other models for core transport and pedestal density \cite{Saarelma_2017, pedestal_prediction}. 

In this study, the focus is predicting the pedestal density, and ML tools analyzed are compared to an experimental log-linear fit published in \cite{Frassinetti_2020}.

\subsection{JET Pedestal Database}
\begin{figure}
	\centering
	\begin{subfigure}{0.35\linewidth}
		\centering
		\includegraphics[scale=0.34]{./src/traingularity_crop_2}
		\caption{}
		\label{subfig:triangularity}
	\end{subfigure}\hfill
	\begin{subfigure}{0.60\linewidth}
		\centering
		\includegraphics[scale=0.22]{./src/MTANH_fit_21}
		\caption{}
		\label{subfig:mtanhfit}
	\end{subfigure}\hfill
	\caption{ \textbf{(a)} Plasma shape parameters from the cross-section of a tokamak. \textbf{(b)} HRTS measurement profiles (blue), which are fitted in real space using the mtanh equation (eq. \ref{eq:mtanh}) then mapped to the normalized poloidal flux coordinate $\Psi_N$ (red).}
	\label{fig:pedestal_db_figs}
\end{figure}

The JET pedestal database contains over 3000 entries, with each entry corresponding to time averaged measurements of various plasma parameters over the course of 70-95\% of an ELM cycle, representing the conditions of the highest pedestal plasma pressure prior to the next ELM.
The measurements are done using high resolution Thomson scattering (HRTS)\cite{Pasqualotto_2004}, and are then fitted using the modified tangent hyperbolic function (mtanh): 
\begin{equation} \label{eq:mtanh}
\text{mtanh}(r) = \frac{h_1 - h_0}{2} \left( \frac{(1 + sx) e^x - e^{-x}}{e^x + e^{-x}} + 1\right) + h_0 , \quad \quad x=\frac{p-r}{w/2}
\end{equation}
where the pedestal height, position, width, and slope inside the pedestal top are $h_1$, $p$, $w$, and $s$ respectively, and $r$ the normalized radius $\Psi_N$ (Fig \ref{subfig:mtanhfit}). 
Since the measurements are taken near the end of the ELM cycle, the pedestal parameters should be saturated near their maximum, right before the ELM.

The key engineering quantities in the database and their units ([-] dimensionless) are listed below:

\begin{wraptable}{r}{0.4\textwidth}
\centering
\caption{Main engineering parameter domains of the filtered dataset.}
\label{tab:main_domain}
\begin{tabular}{| c | c | }
	\hline
	Eng. Param & Domain \\
	\hline
	$I_P$ [MA] & $[0.81, 4.48]$ \\
	$B_T$ [MW] & $[0.97, 3.68]$ \\
	$a$ [m] & $[0.83, 0.97]$ \\
	$\delta$ [-] & $[0.16, 0.48]$ \\
	$M_{eff}$ [-] & $[1.88, 2.18]$ \\
	$P_{NBI}$ [MW] & $[10^{-3}, 32.34]$ \\
	$P_{ICRH}$ [MW] & $[0, 7.96]$ \\
	$P_{TOT}$ [MW] & $[3.4, 38.22]$ \\
	$V_P$ [m$^3$] &  $[58.3, 82.19]$ \\
	$q_{95}$ [-] & $[2.42, 6.04]$ \\
	$\Gamma$ [$10^{22}$ e/s] & $[0, 15.5]$ \\
	$H$ [-] & $[0, 0.18]$ \\
	$P_{SD}$ [$10^6$nbar] & $[0,1000]$ \\
	\hline
\end{tabular}
\end{wraptable}
\begin{packed_enum}
	\item $I_P$ [MA], plasma current, current driven through the plasma that generates the poloidal magnetic field, (Fig. \ref{subfig:triangularity})
	\item $B_T$ [T], toroidal magnetic field, (Fig. \ref{subfig:triangularity})
	\item $R$ [m], major radius of the plasma, (Fig. \ref{subfig:triangularity})
	\item $a$ [m], minor radius of plasma, (Fig. \ref{subfig:triangularity})
	\item $\delta$ [-], triangularity, normalized horizontal displacement of the top/bottom of the plasma from the main axis, (Fig. \ref{subfig:triangularity})
	\item $V_P$ [m$^3$], the plasma volume,
	\item $H$, isotope ratio of fuel, hydrogen / deuterium, 
	\item $M_{eff}$, effective mass of the fueling isotope,
	\item $q_{95}$ [-], safety factor at the 95\% flux surface. Safety factor is the 'windiness' of the magnetic fields in a reactor, i.e., the  number of toroidal circles the helical field line completes within one poloidal revolution. $q$ is called the safety factor as low $q$ plasmas are susceptible to MHD instabilities. Typically, the baseline scenarios operate  $q_{95}$ around 3-4,
	\item $P_{NBI}$ [MW], neutral beam injection heating power,
	\item $P_{ICRH}$ [MW], ion cyclotron radio frequency heating, 
	\item $P_{TOT}$ [MW], total power ($P_{TOT} = P_{NBI}+ P_{ICRH} + P_{OHM} - P_{SH}$, where $P_{OHM}$ is the ohmic heating due to the plasma current, and $P_{SH}$ is the power lost due to the shine through of NBI heating),
	\item $\Gamma$ [ $10^{22}$ electrons per second], gas fueling rate of the deuterium or hydrogen,  
	\item $DC$, the divertor configuration, can take on values of C/C, V/H, V/C, V/V, C/V, C/H, (see \cite{Frassinetti_2020} for more information),
	\item $TW$, the type of wall, as JET was upgraded in the mid 2010s, and moved from having a Carbon wall to an 'ITER like wall' (ILW),
	\item $P_{SD}$ [$10^6$ nbar], the subdivertor pressure.
\end{packed_enum}


For the main engineering parameters, the uncertainties are calculated by taking the standard deviation of the values over the time period in which the measurements were taken. 

There are also global parameters stored in the database, such as $\beta_\theta^{ped}$, $\beta_N$, $Z_{eff}$. However, in this study, we are first considering a model that only utilized machine engineering parameters as input. This assumption is in contrast to EPED that also takes plasma parameters as input, including pedestal density and core  $\beta$ for example. These type of models assume that these plasma parameters can be achieved via actuation of the engineering inputs. 

\begin{comment}
The global parameters stored in the database are listed below: 
\begin{itemize}
	\item $\beta_\theta^{ped}$ [-], $\beta$ is the ratio of plasma pressure $p$ to the pressure exerted by the magnetic field $B$, $\beta = p / B^2 / 2\mu_0$, thus $\beta_\theta^{ped}$ is the pressure due to the poloidal magnetic field $B_\theta$ and plasma electron pressure at the pedestal $p_e^{ped}$
	\item $\beta_N$, normalized $\beta$ for comparison between reactors, as $\beta$ has an inherent limit based on MHD stability, and is a function of the plasma current, minor radius and magnetic field such that $\beta_N = \beta / I / aB$, is commonly known as the Troyon factor
	\item $Z_{eff}$, the effective charge state of the plasma
\end{itemize}

The global parameters are certainly interesting, but within the context of this thesis are not considered to be viable inputs to a predictor, as they rely on information that is unavailable as a control knob on a reactor.
A truly predictive model cannot take plasma parameters as inputs. Today, EPED takes $\beta$, $n_e^{core}$ and $Z_{eff}$ as inputs assuming the feedback can be used to choose the density and $\beta$.
Models like EPED rely on the principle that reactor operators would 'know' these density and beta points are within reachable operational space, and that furthermore they know the recipe to get there.
\end{comment}

A model of interest is one that uses the main engineering parameters to calculate pedestal profile parameters like height, width, or position for the pedestal quantities temperature, density, or pressure.
The pedestal profile parameters stored in the database are determined using the mtanh fit eq.\;(\ref{eq:mtanh}), and the uncertainties are the fit uncertainties from the use of the mtanh function \cite{Frassinetti_2020}.
The fit uncertainties are expected to be significantly smaller than the natural scatter of the data due to the fluctuation of the plasma.

Additionally, the database entries contain the parameter FLAGS, which correspond to the specific setup of an experiment.
Examples of FLAGS contained in the database: main fueling element, usage of RMPs, pellets, or impurity seeding, divertor configuration. 
\begin{comment}
For example, what element the fuel for a shot is, or if resonant magnetic pulses (RMPs) [\textcolor{blue}{source}], pellets [\textcolor{blue}{source}] or impurity seeding [\textcolor{blue}{source}] were used in a shot are all FLAGS contained in the database.
\end{comment}
Each entry in the database is validated either by hand or computationally, and there is a FLAG corresponding to the quality of the HRTS measurement determined by the validation \cite{Frassinetti_2020}.
Only entries that have been validated are used in this thesis.
Shots with impurity seeding are used, as they make up about 600 entries.
To keep the dataset simple, entries with RMPs, pellets, and kicks are excluded, as these are used to manipulate the pedestal for ELM control, mitigation, or suppression \cite{Viezzer_2018}.

After filtering out the shots with RMPs, kicks, pellets, non-validated HRTS, and shots that do not use deuterium, the dataset is reduced to 1888 entries. The final main engineering and pedestal parameter domains are given in Tables \ref{tab:main_domain} and \ref{tab:ped_quant} respectively.
\begin{center}
\begin{table}[h!]
\begin{tabular}{ | c | c | c | c | c | }
	\hline 
	& Height & Width $[\Psi_N]$ & Position $[\Psi_N]$ & Slope [-] \\ 
	\hline
	$n_e^{ped}$ &[1.849, 11.737] ($10^{19}$ m$^3$) & [0.015, 0.173]& [0.953, 1.029] & [$10^{-6}$, 0.188] \\
	$T_e^{ped}$ & [0.149, 1.894] (keV)& [0.013, 0.105] & [0.926, 1.002] & [0.026, 0.502] \\
	$p_e^{ped}$ & [0.808, 17.804] (kPa)& [0.014, 0.099] & [0.931, 1.002]& [0.041, 0.789] \\
	\hline
\end{tabular}
\caption{Domains of pedestal parameters for deuterium shots stored in the JET pedestal database after RMPs, kicks, pellets, and non-validated HRTS are filtered out.}
\label{tab:ped_quant}
\end{table}
\end{center}


\subsubsection{Empirical Analysis}

Empirical analysis of the JET pedestal database was carried out \cite{Frassinetti_2020}, and has yielded a log-linear scaling law for the pedestal density height $n_e^{ped}$:
\begin{equation} \label{eq:scaling}
	n_e^{ped} = (9.9 \pm 0.3) I_p^{1.24 \pm 0.19} P_{TOT}^{-0.34 \pm 0.11} \delta^{0.62 \pm 0.14} \Gamma^{ 0.08 \pm 0.04} M_{eff}^{0.2 \pm 0.2}
\end{equation}
In this thesis, we will focus on the pedestal prediction with ML approaches relative to the performance of the empirical scaling law.
The choice of parameters in the log-linear regression by Lorenzo Frassinetti et. al. \cite{Frassinetti_2020}, was backed by physical intuition on the pedestal density height. Since log-linear regression was used, the scaling law above avoided using cross-correlated variables, which can verified in Figure \ref{fig:emperical}. 

\begin{figure}[h]
        \centering
        \begin{subfigure}{0.5\linewidth}
                \centering
                \includegraphics[scale=0.2]{./src/R_vs_NEPED_matplotlib}
                \caption{}
                \label{subfig:icecream}
        \end{subfigure} \hfill
        \begin{subfigure}{0.45\linewidth}
                \centering
                \includegraphics[scale=0.2]{./src/input_correlations}
                \caption{}
                \label{subfig:corr}
        \end{subfigure}
	\caption{Empirical data plots of the JET pedestal database: \textbf{(a)} correlation without causation between the major radius $R$ and $n_e^{ped}$,  \textbf{(b)}  Correlation matrix of the main engineering parameters. A grey coloring represents no correlation, whereas blue and yellow are negative and positive correlation, respectively.}
	\label{fig:emperical}
\end{figure}

To improve prediction quality, it is useful to include additional inputs from the list of main engineering quantities that were not used in the log-linear scaling.
However, by plotting joint histograms between the control parameters and $n_e^{ped}$, serious questions can be raised regarding which parameters can and should be given to a machine learning model.
For example, the dependence of the pedestal density on the major radius could lead to an early conclusion that with higher values of $R$, a higher pedestal height is achieved (Fig. \ref{fig:emperical}a). 
However, this is a case of causation without correlation, or ice-cream correlation\footnote{Ice-cream correlation refers to the correlation of increasing ice cream sales and increasing number of drownings in Finland during the summer. Although the variables are indeed correlated, higher ice cream sales are not in fact the cause of higher drowning rates, nor vice-versa.}, and the actual culprit of the causation is the Shafranov shift; the outward radial displacement of the magnetic axis from the geometric axis that is prominently found in MCF devices~\cite{shafranov_equilibrium_1963, freidberg_plasma_2007}. The shift increases with normalized pressure, which increases $R$. As normalized pressure increases with pedestal pressure, there is a cross-correlation between pedestal pressure and $R$. For this reason, $R$ is excluded from the list of inputs to the ML models in this thesis, and only when multi-machine databases with significantly larger variation of $R$ are available should it be included.
The divertor configuration on the other hand, does have a real correlation, and can have a large impact on the pedestal. However, the analysis in this thesis only makes use of the numerical parameters available, and thus divertor configuration will not be used as an input parameter, as it is categorical.  

Another engineering parameter that is ignored in the analysis is the sub-divertor pressure $P_{SD}$, which is correlated with the gas fueling rate, but also depends on other main engineering parameters, such as divertor configuration, input power, and wall conditions.
From the filtered dataset, the values of $P_{SD}$ vary widely, with over 200 entries without a measured value of $P_{SD}$.
Because of this volatility, $P_{SD}$ is ignored, however future work may choose to filter the dataset such that inclusion of $P_{SD}$ is possible.



\section{General Machine Learning Analysis}\label{sec:principle-machine-learning-analysis}
Within the context of this thesis, a model refers to a prediction function $f$ that takes any combination of the main engineering parameters as inputs, $\vec{x} = (x_1, x_2, \cdots, x_p)$,  and provides an estimate of the pedestal density height, $\hat{y}$, as well as the uncertainty of the estimate (when applicable).
The prediction quality is quantified through both the root mean squared error (RMSE) and mean absolute error (MAE), since a robust model minimizes both of these.
The RMSE score penalizes predictions that are far away from the ground truth, whereas the MAE uniformly calculates the distance between predictions and the ground truth.
\begin{comment}
\[RMSE = \sqrt{\frac{\sum_i^N \left( y_i - \hat{y}_i \right)^2}{N}} \quad\quad MAE = \frac{\sum_i^N |y_i -  \hat{y}_i |}{N}  \] 
where $N$ is the number of points predicted upon, and $y_i$ is the ground truth value of $n_e^{ped}$ for the $i$'th entry in the dataset. 
\end{comment}
\subsection{Model Fitting and Validation}
To make a prediction, the model must first be \textit{fitted}, which means to learn the parameters $\vec{\theta}$ of the prediction function via a model-specific learning algorithm such that $f = f(\vec{x}: \vec{\theta})$.
In the case of linear regression, the learning algorithm is the ordinary least squares method (OLS), which minimizes the mean squared error in order to find the optimal linear coefficients $\theta(w_i)$ \cite{OLS}.
Not all supervised learning regression algorithms minimize the RMSE or MAE to fit model parameters. 

The performance of the fitted model is scored using data that the model has not observed previously.
If the same data used in the fitting was used to score the model, the model would simply repeat predictions that  it had been fitted with, and would fail to predict useful information on  unseen data.
This is called \textit{overfitting}. To avoid overfitting a common practice in supervised machine learning is to hold part of the available data as a test set.
This can be done by randomly splitting the available data into training and test subsets and by evaluating the model on the test set.
Depending on the performance on the test set, the \textit{hyperparameters} of the model are adjusted to optimize the performance on the test set  (an example of a hyperparameter is the number of trees in a random forest, or learning rate for artificial neural networks).
However, this method can lead to overfitting on the test set, since the hyperparameters can be adjusted until the model performs best on the test set.
In this sense, knowledge about the test set leaks into the model, and evaluation metrics no longer report on general performance.
Additionally, in randomly splitting the data into two groups, there is a new problem of \textit{selection bias}, in which the results are dependent on the random choice of entries contained in the training and test sets \cite{Selection_Bias}.
To overcome these problems, \textit{cross-validation}, or CV, is implemented throughout the analyses in this thesis to validate the parameters and generalization capabilities of a model.
The general approach for \textit{k-fold} CV is to split the dataset into $k$ subsets, and apply the following procedure to each of the k folds:
\begin{itemize}
	\item Use $k-1$ of the folds to fit a model 
	\item Hold out the remaining fold to validate the fitted model
\end{itemize}
Furthermore, \textit{repeated k-fold} CV is employed, in which the above process is repeated $p$ times.
The final performance measure is then the average of the scores on the test sets left out.
This method is very computationally expensive since $k*p$ models are being fit, but it is extremely efficient with the data, while additionally removing selection biases with sufficient folds and repeats.

\subsection{Linear Regression}
A commonly used approach in the plasma physics community is log-linear regression to create scaling laws like that from Lorenzo Frassinetti et. al \cite{Frassinetti_2020}.
A general overview of linear regression can be found in ~\cite{hastie01statisticallearning}. 
Additional details that are used in this thesis are as follows: 
\begin{itemize}
	\item By minimizing the MSE through OLS, the scalar linear coefficients $w_i$ corresponding to the control parameter $x_i$ can be determined, and from these coefficients we learn the linear correlation of an engineering parameter and $n_e^{ped}$, i.e., if the coefficient in front of the plasma current, $w_{I_P}$, is positive, then as $I_P$ increases, so will the prediction of $n_e^{ped}$.
	\item By adding a regularization term ($L^1$ norm of the weights) to the MSE cost function, the coefficients will be minimized as well, resulting in some coefficients becoming zero. From this procedure it is determined whether an engineering parameter is 'useful' in the context of predicting point estimates of $n_e^{ped}$ using linear regressors, and can reduce dimensionality when possible. This is known as LASSO. \cite{Bisong2019, LASSO_OG, LASSO_COIN}
	\item By transforming the point prediction of $n_e^{ped}$  into a normal distribution we can obtain the uncertainty in the prediction. This is done by determining the optimal linear coefficients via maximum likelihood estimation instead of OLS, and thus the coefficients transform from scalars into normal distributions with mean $\mu$ centered around the scalar coefficient and spread $\sigma^2$ representing the uncertainty in the coefficient. This is otherwise known as Bayesian Regression~\cite{bayes_regr}.
\end{itemize}

By using more input parameters than those which are used in the scaling law, eq. (\ref{eq:scaling}), the intent is to achieve a lower RMSE while additionally maintaining interpretability, i.e., attach physical intuition to the coefficients determined by the linear model. However, by including more parameters, we also introduce cross-correlation to the system, and expect that the coefficients determined may vary from those in the scaling law. We do not expect drastic changes from the coefficients determined by a linear regressor using more engineering parameters than those listed in Eq. (\ref{eq:scaling}), i.e., the pedestal density will still increase as the plasma current increases.  The linear models analyzed come from the sklearn and PyMC libraries \cite{scikit-learn, Salvatier2016}. 

\subsection{Gaussian Processes}
In contrast to linear models, Gaussian Processes (GPs) are non-parametric, in that there is no function to be minimized, but rather an optimal set of functions are found that best characterize predicting $n_e^{ped}$ given the engineering parameters as inputs.
More in-depth analysis of GPs can be found in the following sources \cite{gortler2019a, Rasmussen2004, vapnik95}.
The details pertinent to the analysis in this thesis are as follows:
\begin{itemize}
	\item Choice of kernel (covariance function) is normally based on the 'wigglieness' of the functions one is trying to parameterize \cite{cov-matrix-maths}, but with higher dimensional space, this means nothing. Therefore, the kernels desired are those that best predict the pedestal density when optimized.
	\item Prediction uncertainty is built into GPs as the joint-Gaussian group of functions determined through optimization of the kernel will give predictions of the pedestal density that are averaged for the point prediction, and the standard deviation is the uncertainty \cite{Rasmussen2004}. 
	\item Sensitivity analysis is used to determine the relevant engineering parameters for GPs, from which the dimensionality of the input space can be reduced if parameters are deemed irrelevant. Three different forms of sensitivity analysis are used and are described below \cite{pmlr-v89-paananen19a}. 
	\item There are two approaches to utilize the measurement uncertainties given in the database: (a) a fixed noise kernel is added on to the base kernel such that the measurement uncertainties are additive to the input space, (b) transforming the GP model from homoscedastic to heteroscedastic, where the homoscedastic model assumes constant Gaussian noise and the heteroscedastic takes noise values that vary for each input entry. Furthermore, the heteroscedastic model attempts to learn the uncertainty space given uncertainty of the input, i.e., both the mean and \textit{local} variance of $n_e^{ped}$ are estimated~\cite{heteroscedastic}.   
\end{itemize}

GPs scale unfavorably with increasing input space size, therefore by using sensitivity analysis, the intent is to remove engineering inputs if they do not improve the prediction capability of the GP model.

Three types of sensitivity analysis are used:
\begin{itemize}
	\item Automatic Relevance Determination (ARD): The predictive relevance of each input variable is inferred from the inverse of the length-scale parameter associated with that variable within the kernel. A large length scale (infinite, for example) means that there is no correlation between the latent space and the variable in question, and thus the relevance would be zero \cite{pmlr-v89-paananen19a}. 
	\item Kullback-Leibler Divergence (KLD): The KLD is a well known measure of dissimilarity between two probability distributions, and is a function of both the latent mean and uncertainty of each distribution \cite{KLD}. In this case, the input space is 'shifted' via perturbing values of individual variables, and the KLD of the resulting new latent space is measured against the unperturbed case. A large change in the KLD indicates that the single variable that was perturbed has high prediction relevance \cite{pmlr-v89-paananen19a}. 
	\item Variance of the Posterior (VAR): The same method of perturbation applies, but instead of calculating the KLD, variability in  only the latent mean of the fitted GP is calculated \cite{pmlr-v89-paananen19a}.
\end{itemize}

The Gaussian Process models used in this analysis are adapted from the GPy library \cite{gpy2014}. 

\subsection{Random Forests}
Another popular non-linear model is the ensemble of decision trees \cite{DT_OG} that is the Random Forest(RF). General information on  Random Forests can be found in the following sources \cite{RF_OG, 598994}, but the details pertinent to the thesis are stated below:
\begin{itemize}
	\item RFs are fitted using bootstrap aggregation (bagging). Each decision tree within the forest is fit from a 'bag' of random samples drawn from available training entries, meaning not every tree will see every available training sample, allowing for the calculation of the average error for each sample using the predictions of trees that do not contain the sample in their bag. This allows us to approximate how many decision trees to use in the forest, as the OOB error will eventually stabilize. The bag consists of a predetermined number of features which are also randomly sampled, which allows for the determination of the optimal number of inputs to sample, as well as which inputs are optimal ~\cite{hastie01statisticallearning}.
	\item UQ in prediction can be determined by taking the standard deviation of predictions from all of the decision trees that make up the forest. 
	\item A variant of RFs called Extremely Randomized Trees (ERTs) will also be compared. The two main differences between RFs and ERTs are (a) decision trees in ERTs sample entries for their bags without replacing them such that no decision tree contains any of the same entries and (b) nodes in decision trees are split based on different criteria; RFs convert parent nodes into two homogeneous nodes by choosing the split that minimizes the MSE, whereas ERTs convert the parent node into two child nodes via a random split~\cite{geurts_extremely_2006}.
\end{itemize}

Random Forests and Extremely Randomized Trees offer little interpretability in comparison to parametric models, but by quantifying how much the impurity of a node decreases (a pure node has no child nodes) with the probability of reaching that node, the relative importance of the feature housed in the node is determined. Using this, we can get insight into which features are driving the predictions of $n_e^{ped}$ for RFs and ERTs. The RFs and ERTs used in this analysis are adapted from the sci-kit learn library \cite{scikit-learn}. 

\subsection{Artificial Neural Networks}

Numerous previous studies have investigated the reasons why artificial neural networks (ANNs) work \cite{silver_mastering_2016, 7333916, CALTECH}.  The ANNs used in this thesis are all fully-connected feed-forward networks \cite{Goodfellow-et-al-2016, SCHMIDHUBER201585}. Work has already started on using feed-forward ANNs in predicting pedestal quantities \cite{Andreas}. 
Since ANNs are very delicate, the primary goal in this thesis is to probe the hyperparameter and architecture spaces for future research upon which to build. We optimize the following hyperparameters: mini-batch size, learning rate, number and size of hidden layers, activation functions, layer types, length of training, and regularization. The optimal hyperparameters are those that achieve the lowest RMSE through the cross-validation process.

In this thesis, the prediction uncertainty of ANNs is determined through grouping many ANNs of similar hyperparameters into an ensemble such that the standard deviation of the predictions of each network in the ensemble is the uncertainty in the ensemble prediction.

The ANNs used in this analysis are built using PyTorch~\cite{NEURIPS2019_9015}. 


\subsection{Meta-Modeling}
Another use of the uncertainties stored within the database is for a given entry to generate normal distributions with the mean of a parameter value and spread of the uncertainty. By sampling from the generated distribution, it is possible to create new 'synthesized' entries.
As seen in the next section, many models can predict well for $n_e^{ped} \leq 10$, but struggle for densities higher than that.
By including synthesized values in the fitting procedure, the hope is that models would be able to better predict higher $n_e^{ped}$ values.
\vspace{7cm}

\section{Results}{\label{sec:results_1}
Each model analyzed is fit using the following list of main engineering parameters as inputs:
\begin{itemize}
        \item $I_p, \; B_T,\; a, \; \delta, \; M_{eff},\; P_{NBI},\; P_{ICHR},\; P_{TOT}, \; q_{95},\; \Gamma, \; H,\; V_P$.
\end{itemize}
There are cross-correlations among these parameters (Fig. \ref{fig:emperical}), and, as will be observed, some models do not perform very well due to these correlations. 
Through the use of cross-validation, the hyperparameters of each model are tuned such that optimal performance is achieved on the average performance on each fold subset.
The relevant hyperparameters, and how they were determined, is discussed in each individual model subsection, as well as the effect of meta-modeling when applicable.
Table \ref{tab:performance_models} presents the final performance metrics for each optimized model. 

\begin{table}[h]
	\begin{center}
		\begin{tabular}{>{\rowmac}c|>{\rowmac}c|>{\rowmac}c<{\clearrow}}
		\hline
		Model & RMSE & MAE \\
		\hline
		Scaling Law & $0.9203 \pm 0.63$ & $0.7189 \pm 0.63$ \\
		\hline
		Linear & $0.8166 \pm 0.0605 $ & $0.5956 \pm 0.0379$ \\
		\setrow{\bfseries}GP & 0.4566 $\mathbf{\pm}$  0.0217 &  0.3395 $\mathbf{\pm}$ 0.01383\\
		RF & $0.5938 \pm 0.0352$ & $0.4225 \pm 0.0191$ \\
		ERT &$0.5623 \pm 0.0368 $ & $0.3927 \pm 0.0199$ \\
		ANN & $0.6126 \pm 0.0694$ & $0.4418 \pm 0.0421$ \\
		\hline
		\end{tabular}
		\caption{The best RMSE and MAE of the predictions from each optimal model are calculated by averaging the results across each fold and repeat of the repeated cross-validation method. Uncertainty in the calculated RMSE and MAE is derived from the  standard deviation of the RMSE across each fold.}
		\label{tab:performance_models}
	\end{center}
	\vspace{-38pt}
\end{table}

\subsection{Linear Regression}

Using a Bayesian linear regression model without an intercept using the control parameters as inputs, the RMSE and MAE improve (for $n_e^{ped} < 11 \times 10^{19} \text{m} ^{-3}$) by including more parameters than in the reference scaling law, eq. (\ref{eq:scaling}) (Fig. \ref{fig:lin_reg}).
\begin{figure}[hb!]
	\centering
	\includegraphics[scale=0.18]{./src/bayes_comp}
	\caption{Comparison of a Bayesian Ridge regressor fit using all available engineering parameters against the log-linear scaling law eq.\;( \ref{eq:scaling}). \textbf{(a)} The predictions of a Bayesian regressor (green) vs the scaling law (orange) with the ground truth being the black dotted line. \textbf{(b)} The Residual comparison of the Bayesian regressor (green) and the scaling law (orange) }
	\label{fig:lin_reg}
\end{figure}
\begin{comment}
\begin{figure}[hb!]
	\centering
	\begin{subfigure}{0.49\linewidth}
		\centering
		\includegraphics[width=6.5cm]{./src/Bay}
		\caption{}
		\label{subfig:bayes_pred}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\centering
		\includegraphics[width=6.5cm]{./src/linear_comp_v3}
				\caption{}
		\label{subfig:bayes_resid}
	\end{subfigure}
	\caption{Comparison of a Bayesian Ridge regressor fit using all available engineering parameters against the log-linear scaling law eq.\;( \ref{eq:scaling}). \textbf{(a)} The predictions of a Bayesian regressor (green) vs the scaling law (orange) with the ground truth being the black dotted line. \textbf{(b)} The Residual comparison of the Bayesian regressor (green) and the scaling law (orange) }
	\label{fig:lin_reg}
\end{figure}
\end{comment}
Due to cross-correlated variables, we observed that LASSO did not perform well in reducing the dimensionality of the input space. The LASSO method deemed the following features as 'unimportant' (their coefficients dropped to zero or near zero): $H, M_{eff}, B_T, V_P$. A new model with reduced dimensionality was fit by removing the unimportant features from the input space and the prediction quality decreased dramatically with any reduction of dimensionality; in removing H or $M_{eff}$ from the input space, the RMSE and MAE increased by 0.3, while when removing $V_P$ or $B_T$, there was an increase of 1.5 in the RMSE and MAE. 
\begin{comment}
The results of LASSO are slightly misleading, as linear regressors and their regularized extensions are prone to problems when working with many correlated variables and many of the variables in the input space are correlated (e.g., $P_{TOT}, P_{NBI}, P_{ICRH}$), which may be why the best RMSE and MAE are achieved when all variables are included.
\end{comment}

%------------------------------------------
\begin{wraptable}{r}{0.36\linewidth}
\centering
\caption{Coefficients determined by Bayesian Linear Regression. Each coefficient is a normal distribution with mean $\mu$ and spread $\sigma^2$}\label{tab:new_coef}
\begin{tabular}{ | c | c | c |}
			\hline
			Feature & $\mu$ & $\sigma^2$ \\
			\hline
			$I_p$ & 0.15 & 0.06 \\
			$B_T$ & 0.956 & 0.072 \\
			$a$ & 2.966 & 0.479 \\
			$\delta$ & 12.95 & 0.154 \\
			$V_P$ & -0.05 & 0.007 \\
			$q_{95}$ & -1.064 & 0.0542 \\
			$P_{NBI}$ & $-1.911$ & 0.0546 \\
			$P_{ICRH}$ & -1.976 & 0.0561 \\
			$P_{TOT}$ & 1.926 & 0.0557 \\
			$\Gamma$ & 0.125 & 0.007 \\
			$H$ & -4.016 & 0.374  \\
			$M_{eff}$ & 1.369 & 0.053 \\
			\hline
\end{tabular}
\end{wraptable}
%------------------------------------------
Fitting a Bayesian Ridge regressor using all of the available engineering parameters yielded new coefficients and uncertainties therewith (Table \ref{tab:new_coef}). It is observed that the coefficients for $P_{TOT}$ compete with $P_{NBI} \text{ and }  P_{ICRH}$. 
From Table \ref{tab:main_domain} we know that there are plasmas with $P_{NBI}$ 100\% of $P_{TOT}$, so the regressor balances the two out by choosing $P_{TOT}$ and $P_{NBI}$ to have opposite signs.     
From the coefficients and their uncertainties, the general uncertainty in the point prediction of $n_e^{ped}$ can be ascertained. The uncertainties were normally distributed and range from 1.64 to 1.8 $ \times 10^{19}\text{m}^{-3}$. Having high uncertainty is good when the prediction is far from the ground truth (high residual), but for predictions on $n_e^{ped} \geq 8.5 \times  10^{19}\text{m}^{-3}$ the uncertainty no longer covers the residual. 
Just like the scaling law, the predictions from the Bayesian linear model taper off at around $n_e^{ped} \geq 8.5 \times 10^{19}\text{m}^{-3}$, which suggests that parametric models like linear regressors are generally unable to capture the non-linear complexity of higher pedestal density heights from the given set of input parameters.


\subsection{Gaussian Process}

Sensitivity analysis of GPs yielded the following variable importance ranking (Fig. \ref{fig:GP_dim}): 
\begin{itemize}
	\item  $\delta, \; a,\; I_p,\; V_P, \; P_{NBI}, \; \Gamma, \;  P_{TOT},\;  q_{95},\;  P_{ICRH},\; B_T,\; H,\; M_{eff}$. 
\end{itemize}
To determine these results, a GP model with a radial basis function (RBF) kernel~\cite{kernel_cookbook} with added constant bias term was fit using all main engineering parameters, and the relevance of each variable is calculated using ARD, KLD, and VAR. This process is repeated 5 times and the results over all 5 are averaged. Each method gives relatively similar results and suggest that the correlated variables $H$ and $M_{eff}$ do not aid in predicting the pedestal density (Fig. \ref{fig:GP_dim}a). This may be due to the filtered dataset, which was exclusive in its values for $H$ and $M_{eff}$ (only deuterium experiments were considered). This is expected to change if a wider range of fueling elements was included in the dataset, and in future work we do not expect $M_{eff}$ to rank as low as it does. Each sensitivity analysis also ranks $B_T$ low, which is most likely due to the inherent correlation between $B_T, \; q_{95}, \text{ and } I_P$, as most of the information of $B_T$ is contained within $q_{95} \text{ and } I_P$. 
\begin{comment}
\begin{itemize}
	\item \textbf{ARD}: $\delta, a, V_P,  I_p, P_{ICRH}, \Gamma, P_{NBI},  B_T, P_{TOT}, q_{95}, H, M_{eff}$
	\item \textbf{KLD}: $\delta, a,  I_p, P_{NBI}, V_P, P_{TOT},  q_{95}, \Gamma, B_T, P_{ICRH}, H, M_{eff}$
	\item \textbf{VAR}: $\delta, a, I_p, \Gamma, V_P, P_{NBI}, q_{95}, P_{ICRH}, P_{TOT}, B_T, H, M_{eff}$
\end{itemize}
\end{comment}

\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{./src/GP_sensitivity_analysis_final_v2}
	\caption{Steps towards dimensionality reduction through the use of Gaussian Processes. \textbf{(a)} The result of sensitivity analysis as well as the average of the three types used plotted in the dashed black line. \textbf{(b)} The dimensionality order of input variables comes from their ranking via the average of the three sensitivity analyses (dashed black line in diagram to the left). For each kernel, a GP model is fit using cross-validation (5 folds, 5 repeats) for each additional dimension of data, starting with 1d input of $\delta$, followed by 2D input of $\delta, a$ and so on. The RMSE is calculated on folds left out and averaged across all folds. } \label{fig:GP_dim}

\end{figure}
\begin{comment}
\begin{figure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=8.5cm]{ ./src/GP_sensitivity_analysis_final_v1_scaled}
				\caption{}
		\label{subfig:GP_sens}
	\end{subfigure}\hfill
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=8.5cm]{ ./src/final_GP_dim_v1_scaled_words}
				\caption{}
		\label{subfig:GP_dimens}
	\end{subfigure}
	\caption{Steps towards dimensionality reduction through the use of Gaussian Processes. \textbf{(a)} The result of sensitivity analysis as well as the average of the three types used plotted in the dashed black line. \textbf{(b)} The dimensionality order of input variables comes from their ranking via the average of the three sensitivity analyses (dashed black line in diagram to the left). For each kernel, a GP model is fit using cross-validation (5 folds, 5 repeats) for each additional dimension of data, starting with 1d input of $\delta$, followed by 2D input of $\delta, a$ and so on. The RMSE is calculated on folds left out and averaged across all folds. } \label{fig:GP_dim}
\end{figure}

\end{comment}
\begin{comment}
\begin{figure}
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=0.23]{ ./src/MLP_residual}
				\caption{}
		\label{subfig:MLPW_RES}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=0.23]{ ./src/MLP_uq}
		\caption{}
		\label{subfig:MLP_UQ}
	\end{subfigure}
	\caption{Three different methods of uncertainty quantification are compared for a GP with an MLP kernel. \textbf{(a)} The true values of $n_e^{ped}$ are grouped into 11 equal-sized bins and the distance between the predictions of those values and the true values are calculated. The residuals are averaged across each bin and compared between the methods. \textbf{(b)} The same binning procedure, but with averaging the prediction uncertainties of the same three methods are compared.}
	\label{fig:MLP_UQ}
\end{figure}
\end{comment}

\begin{figure}
	\centering
	\includegraphics[scale=0.2]{./src/MLP_comp_v2}
	\caption{Three different methods of uncertainty quantification are compared for a GP with an MLP kernel. \textbf{(a)} The true values of $n_e^{ped}$ are grouped into 11 equal-sized bins and the distance between the predictions of those values and the true values are calculated. The residuals are averaged across each bin and compared between the methods. \textbf{(b)} The same binning procedure, but with averaging the prediction uncertainties of the same three methods are compared.}
	\label{fig:MLP_UQ}
\end{figure}


Using the results from the sensitivity analysis, it was found that $H$ and $M_{eff}$ do not aid in improving predictive quality of GPs and that the Multi-Layer Perceptron (MLP)~\cite{gpy_MLP} and Rational Quadratic (RQ)~\cite{kernel_cookbook} kernels perform the best (Fig. \ref{fig:GP_dim}b).  We observe that the MLP and RQ do not improve after $B_T$ is added to the input space, thus confirming the results of the sensitivity analysis (Fig. \ref{fig:GP_dim}b). This means that regarding the current dataset, it is unnecessary to supply $H \text{ and } M_{eff}$ to a GP, which will reduce the computation time when fitting. However, this reduction may be subject to change, since, as stated above, the dataset used was exclusive in its choice of $H$ and $M_{eff}$; thus for databases with multiple hydrogen isotopes included, the importance of these two variables may be more than what was found here. For example, if shots with hydrogen fueling were used, the domains of $M_{eff}$ and $H$ would expand to $[1.0, 2.0]$ and  $[0.0, 1.0]$, respectively, which could potentially change the results of the sensitivity analysis.  Nevertheless, the remainder of the GP models that are analyzed in this section do not use $H$ or $M_{eff}$ during the fitting procedure. 

 

Through uncertainty propagation it was observed that a heteroscedastic model was able to map the relative measurement uncertainties into its latent posterior at the cost of decreased prediction quality (Fig.  \ref{fig:MLP_UQ}). 
To determine the effect of uncertainty propagation, the two approaches described in Section 2 are applied; (a) a fixed kernel with $n_e^{ped}$ measurement uncertainties along the diagonal is added to a base kernel, (b) a heteroscedastic model fixes the built-in noise variance component of GPs for each input entry to be the entries respective measurement uncertainty of $n_e^{ped}$ such to learn the latent space of the used uncertainty. This process is done for both the RQ and MLP kernel and compared to the homoscedastic models.
The homoscedastic slightly outperforms the heteroscedastic model (Fig. \ref{fig:MLP_UQ}). However, the uncertainty of the homoscedastic model is on average 600\% higher than that of the heteroscedastic model. As the heteroscedastic MLP model attempts to learn the uncertainty space, the predictions for $n_e^{ped} > 10 \times 10^{19}\text{m}^{-3}$ are further away from the ground truth than the other models, however, its uncertainty is much lower and similar to that of the propagated uncertainty. Since heteroscedastic GP learns the propagated uncertainties, the generated prediction functions end up being much closer to each other, resulting in a lower variance in prediction. This is very different from the homoscedastic and fixed models, which although performing very well, suffer from having their prediction-generating functions be far apart (variance $ \geq \pm 1.2$), regardless of prediction accuracy. The heteroscedastic model tends to be overconfident at higher densities. However, the uncertainties closely match that of the actual local uncertainty, which is the goal of a heteroscedastic model (Fig. \ref{fig:MLP_UQ}).

\begin{comment}
\begin{wrapfigure}{r}{0.45\linewidth}
	\caption{}	
	\centering
	\vspace{-14pt}
	\includegraphics[scale=0.2]{./src/GP_preds_impurity}
	\vspace{-20pt}
	\label{fig:GP_impure}
\end{wrapfigure}

We also compare how the GPs predict on each type of impurity as seen in Figure \ref{fig:GP_impure}. It is well known that Nitrogen (7.0) and Carbon (6.0) play a large roll in the pedestal, and therefore models could in general have the most trouble in predicting these quantities [\textcolor{blue}{SOURCE}]. On the other hand, Neon (10.0) and Argon (18.0) also play a roll on the pedestal (although for different reasons than Nitrogen and Carbon), yet the GPs are able to predict more accurately on those. Nearly three-quarters of the entries in the filtered dataset are unseeded (0.0), and of the remaining third, about 80 percent have Neon seeding, and even fewer of the rest (for example only 3 entries have Argon or Oxygen (8.0) seeding). Even with the equally few amount of entries for both Carbon, Nitrogen, Argon and Neon, the GP models are still able extrapolate the relations of Argon and Neon better than the entries with Carbon and Nitrogen. This suggests that still there exist pedestal dependencies on Carbon and Nitrogen seeding that current GP models can not extrapolate.
\end{comment}

\subsection{Random Forests}

The optimal number of decision trees and features to sample to utilize in RFs and ERTs was determined using the out-of-bag error (Fig. \ref{fig:RF_exp}). 
\begin{figure}[h!]
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=0.19]{./src/RF_oob_error}

	\end{subfigure}
	\begin{subfigure}{0.5\linewidth}
		\centering
		\includegraphics[scale=0.19]{./src/ERT_oob_error_v2}
	\end{subfigure}
	\caption{OOB error vs number of decision trees of \textbf{(a)} Random Forests \textbf{(b)} Extreme Random Trees. No cross validation is necessary for this procedure, since the process of bagging inherently prevents all trees from seeing the same data. The colored lines correspond to different RFs and ERTs which vary by the amount of features to sample when splitting a node.}
	\label{fig:RF_exp}
\end{figure}

We see that after 254 decision trees in the RF, the OOB error does not improve, i.e., the forest reaches its maximum generalization capability at 254 decision trees. Furthermore, the optimal number of features to sample is 5. This is completely different from what occurs with the ERTs, where the optimal number of features to sample is 12 (all), and the number of trees is 142. The larger number of features sampled by the ERT compared to the RF is most likely due to the random splitting of nodes that ERTs make use of in creating their trees, such that they need to make use of all the features in order to generalize better, whereas the RF aims to minimize the MSE with their splits, thus not requiring all the inputs for an optimal split. For both models, overfitting begins to occur as more trees are added past the minimum OOB, and although the RMSE may improve, the generalizability does not.


\begin{wrapfigure}{r}{0.5\textwidth}
		\centering
		\vspace{-18pt}
		\caption{Effect of meat-modeling in RF and ERTs. The \# of synthesized samples added into dataset is plotted against the resulting RMSE of a model fitted using the synthesized samples.}
		\includegraphics[width=\linewidth, keepaspectratio=true, trim=0 0 0 30, clip]{./src/ERT_vs_RF_meta_modeling}
		\vspace{-22pt}
		\label{fig:RF_meta_model}
\end{wrapfigure}
It is clear that the individual predictors in the random forest do not vary as much as that of the homoscedastic/fixed Gaussian Process models (Figs. \ref{fig:RF_exp} and \ref{fig:RF_Preds}).
Additionally, the uncertainties are generally around equal to that of the residual for the corresponding bin. The model is able to more or less provide an uncertainty that covers its residual, as desired. This does not hold for $n_e^{ped} \geq 10.0 \times 10^{19}\text{m}^{-3}$, but it is certainly within the ballpark. 

\begin{comment}
Similar to GPs, the RFs and ERTs rank $H \text{ and } M_{eff}$ as unimportant, however reducing dimensionality did not improve prediction quality. Tree methods favor features with high cardinality, therefore $H$ and $M_{eff}$ are not as favourable since the fitting dataset contains only deuterium shots. In contrast to GPs, RFs and ERTs do not favor the power variables, $P_{NBI}, P_{TOT}, P_{ICRH}$, since they are cross-correlated. 
\end{comment}


Meta-modeling had generally no effect on RFs and ERTs (Fig.~\ref{fig:RF_meta_model}).
Although only up to 500 meta-model samples are visualized in Figure~\ref{fig:RF_meta_model}, the sporadic bouncing of the RMSE between 0.57 and 0.58 repeats for when even 2000 synthesized entries are added!
This suggests that in order to minimize the MSE of predictions across the entire dataset, RFs and ERTs ignore the additional entries and we conclude that meta-modeling has little to no effect on ERTs and RFs.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.2]{./src/Tree_Comp}
			\caption{Comparison of predictions and uncertainties between Random Forests (green) and Extreme Random Trees (blue). \textbf{(a)} The residual between predictions and the true values,\textbf{(b)} The prediction uncertainties on the same bins of the residuals. }
	\label{fig:RF_Preds}
\end{figure}

\begin{comment}
\begin{figure}[h!]
		\centering
		\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=0.23]{ ./src/Tree_residual}
				\caption{}
		\label{subfig:RF_RES}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=0.32]{ ./src/Tree_uncert}
		\caption{}
		\label{subfig:RF_UQ}
	\end{subfigure}
		\caption{Comparison of predictions and uncertainties between Random Forests (green) and Extreme Random Trees (blue). \textbf{(a)} The residual between predictions and the true values,\textbf{(b)} The prediction uncertainties on the same bins of the residuals. }
	\label{fig:RF_Preds}
\end{figure}
\end{comment}
\vspace{2cm}
\subsection{Artificial Neural Networks}

\begin{wrapfigure}{r}{0.5\linewidth}
	\centering
	\includegraphics[scale=0.2]{./src/ANN_performace_size_v2}
	\caption{Comparison of the size of an ANN and its predictive capability. Each point represents an ANN which hidden layers are equal in size.}
	\label{fig:ANN_dim}
\end{wrapfigure}
Shallow ANNs ($3-5$ hidden layers) outperform larger ANNs (Fig. \ref{fig:ANN_dim}). Therefore, the hyperparameter optimization process is focused on these networks. Since the main engineering parameters vary in magnitude (scalar value of $P_{TOT}$ is much greater than that of $q_{95}$), each parameter is scaled such that it has a mean of 0 and standard deviation of 1. Throughout the hyperparameter optimization process, each model was trained and tested using the repeated cross-fold validation method, with 5 folds and 5 repeats, and the average RMSE of predictions on the left out sets is the overall performance of the ANN. The first hyperparameters to be optimized are the learning rate (LR) and mini-batch size (MBS), and using random search, the optimal MBS and LR were found to be 396 and 0.004 respectively. Considering the dataset is around 2000 entries, the MBS is relatively large, while the LR could be considered very small for such a large MBS. However, since each training epoch, the data is randomly shuffled, it is possible that the training samples in each batch 'compete' with each other's gradient. For example, the training samples of $n_e^{ped} \geq 9.5 \times 10^{19}\text{m}^{-3}$ pull the model weights in an opposite direction than that $n_e^{ped} \leq 9.5 \times 10^{19}\text{m}^{-3}$. The gradient updates applied by the learning rate for this mini-batch size seemed to balance the effect of the training samples, thus resulting in the best training/testing performance. 

Then, via a grid search (across all available activation layers offered by PyTorch), the optimal activation function was determined to be ELU (Exponential Linear Unit), a close cousin to the well known ReLU (Rectified Linear Unit)~\cite{pytorch_act}. Both are ridge functions that act on a linear combination of the input variables, but since they are applied element-wise (for each node in each layer), they are non-linear. Since the above tools like GPs and RFs are non-linear models, it makes sense that a non-linear activation function performs the best. 

The initial ansatz of optimal hidden layers was determined to be either 3, 4 or 5, with between 1000-2000 total nodes (split between each of the hidden layers) (Fig. \ref{fig:ANN_dim}). This criteria was used as a space for further architecture search via random search, while using the optimal MBS, LR and activation function during the search. 
The optimal sizes of each layer for 3, 4, and 5 hidden layer networks is listed below: 
\begin{itemize}
	\item \textbf{3 Hidden Layers}: 483, 415, 254
	\item \textbf{4 Hidden Layers}: 636, 537, 295, 261
	\item \textbf{5 Hidden Layers}: 390, 484, 678, 290, 284
\end{itemize} 
The network with 4 hidden layers performed the best, with an optimal RMSE of $0.6596 \pm 0.023$. 
It was generally seen that shallow networks ($\leq 4$ hidden layers) with steadily decreasing layer size performed the best, whereas the larger networks performed best with this 'bell' shaped size, as seen in the 5 hidden layer network above. 

In ensembles it was observed that the prediction uncertainty grows with increasing ensemble size, while the prediction quality generally did not change (Fig. \ref{fig:ANN_UQ}). The ensembles were comprised of the top performing 3 layer ANN, each network in the ensemble having different initial weights, and the prediction and uncertainty were compared across varying ensemble sizes. As the ensemble size grows, there is a slight decrease in RMSE and thus improved prediction quality (Fig. \ref{fig:ANN_UQ}). However, this comes with the cost of higher prediction uncertainty, where the uncertainty in the ensemble with 15 ANNs has nearly double the uncertainty for that of 5 ANNs. Similar to GPs, the prediction uncertainty of the ANN ensembles is over 300\% the prediction itself. However, unlike GPs, the prediction uncertainty increases with higher pedestal densities. 

Meta-modeling as an additional form of UQ did seem to improve predictions for high $n_e^{ped}$, while sacrificing the overall performance.
This was unique for the ANNs, as all other usages of meta-modeling for other ML tools either did not affect the model or had only adverse effects to performance.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.2]{./src/ANN_comp}
	\caption{A three layer feed forward fully connected ANN with layer sizes 483, 415, 254 was transformed into ensembles of sizes 5 (blue), 10 (orange, 15 (green) and compared: \textbf{(a)} The residual between predictions and the true values, \textbf{(b)} The prediction uncertainties on the same bins of the residuals. }
	\label{fig:ANN_UQ}
\end{figure}

\begin{comment}
\begin{figure}
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=7cm]{./src/ANN_residual}
				\caption{}
		\label{subfig:ANN_res}
	\end{subfigure}
	\begin{subfigure}{0.5\linewidth}
		\centering
		\includegraphics[width=7cm]{./src/ANN_uncert}
				\caption{}
		\label{subfig:ANN_uncert}
	\end{subfigure}
	\caption{A three layer feed forward fully connected ANN with layer sizes 483, 415, 254 was transformed into ensembles of sizes 5 (blue), 10 (orange, 15 (green) and compared: \textbf{(a)} The residual between predictions and the true values, \textbf{(b)} The prediction uncertainties on the same bins of the residuals. }
	\label{fig:ANN_UQ}
\end{figure}
\end{comment}
\begin{comment}
\begin{figure}
	\begin{subfigure}{0.49\linewidth}
		\centering 
		\includegraphics[scale=0.3]{./src/ANN_1}
		\caption{}
		\label{subfig:ann_1}
	\end{subfigure}
	\begin{subfigure}{0.5\linewidth}
		\centering
		\includegraphics[scale=0.3]{./src/ENSEMBLE_5}
		\caption{}
		\label{subfig:ensemble}
	\end{subfigure}
\end{figure}
\end{comment}
\subsection{Under-performing ML Methods}
Many different machine learning models not listed in the above analyses were initially tested, but due to their under-performance compared to RFs/GPs/ANNs, they were abandoned before delving deeper into them. For example, Nearest Neighbor methods like K-nn and R-nn~\cite{Mucherino2009} were tested and abandoned. R-nn especially so, which although they could overfit extremely well with sufficiently small radius, they can not generalize for data entries which they are not fitted with, most often providing no prediction. Support vector machines (SVMs) \cite{hastie01statisticallearning} were tested, but due to the difficulties of deriving the prediction uncertainty and general under-performance, they were dropped. Additionally, SVMs work well when there is a clear separation between regression points, but as we have seen, for $n_e^{ped} \geq 9 \times 10^{19}\text{m}^{-3}$ there is no clear cut discrepancies, so SVMs offer little to no utility. Additionally, it is difficult to obtain uncertainties in predictions when using neighborhood methods and SVMs, and one of the goals of this thesis was to analyze models that could provide uncertainties. Other ensemble methods like AdaBoost~\cite{schapire2013explaining} performed equally or slightly worse than RFs and ERTs, but some scope was needed in this thesis, so they were ultimately dropped from analysis. The combination of multiple model types into an ensemble (e.g. voting ensemble of ERT, RF and GPs) proved only slightly beneficial, and may be looked into further in future research. 
\section{Conclusion and Outlook}

It is clear that major improvements towards predicting the pedestal density height is achieved through the use of non-linear machine learning models while only using main engineering parameters.\footnote{The code developed for the experiments, figures, and scorings used in this thesis, can be found in (and reproduced via) \url{https://github.com/fusionby2030/bsc_thesis}}

The models analyzed are ranked via their respective performance on unseen data through the use of cross validation (Table \ref{tab:performance_models}).
Each ML tool analyzed outperformed the scaling law in predicting $n_e^{ped}$. The root mean squared error and mean absolute error scores for Gaussian Processes, Random Forests, Extreme Random Trees, and Artificial Neural Networks are quite similar, ranging between 20\% of each-other. Strictly speaking, the homoscedastic Gaussian Process model with an MLP kernel achieved the lowest root mean squared and mean absolute error. All models could predict well on pedestal densities less than $9.5 \times 10^{19}\text{m}^{-3}$, however they each failed to extrapolate well on densities greater than $9.5 \times 10^{19}\text{m}^{-3}$, with each model performing 300\% worse on high density points.
\begin{comment}
The non-linear models performed worse on Carbon and Nitrogen seeded entries than they did for other impurity seedings, therefore future work would include further independent analysis into why that happens.  
\end{comment}

For each model, the prediction uncertainty was ascertained. The prediction uncertainty for Random Forests and Extreme Random Trees, aptly covered the residuals of their predictions without being 'overly cautious', while the homoscedastic and fixed kernel GPs generally held relative constant with high uncertainties for each prediction. We were able to utilize the measurement uncertainties of $n_e^{ped}$ in the JET pedestal database by propagating them into a heteroscedastic Gaussian Process model, and were ultimately able to (roughly) map the latent uncertainty space. This could prove useful should these machine learning models be used as surrogate models in simulations.

The effect of meta-modeling was limited to RFs, ERTs, and ANNs but from this we were able to ascertain that there is no adverse effects on ANNs and no effects on RFs and ERTs. Future work would include using meta-modeling on GPs and linear regressors. 

The pedestal is key to high performance H-mode operation. Machine learning, in the future, could provide fast predictions for plasma scenarios. In this work, application of ML tools for $n_e^{ped}$ predictions using the JET pedestal database was done. Future work should expand these studies to other pedestal parameters, such as $T_e^{ped} \text{ or } p_e^{ped}$ as well as the other features of the pedestal profile (slope at pedestal top, position, width). Future work could also consider combining numerical and experimental data through Bayesian methods such as described in \cite{WU2018417}. 
\newpage

\bibliographystyle{unsrt}
\bibliography{biblioski}
\end{document}
