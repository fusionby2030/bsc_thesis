
%! Author = adam
%! Date = 16.04.21

% Preamble
\documentclass[a4paper, twoside, final, 12pt]{article}

% Packages
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[a4paper, total={170mm,257mm}, left=20mm, top=20mm]{geometry}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{floatrow}
\usepackage{adjustbox}
\usepackage{wrapfig}
\graphicspath{{./src/ }}
\usepackage{caption}
\usepackage{float}
\floatstyle{plaintop}
\restylefloat{table}
\usepackage{tabularx}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\newcommand\setrow[1]{\gdef\rowmac{#1}#1\ignorespaces}
\newcommand\clearrow{\global\let\rowmac\relax}
% Page Setups

% Document


\title{
	{Machine Learning Analysis on the JET Pedestal Database } \vspace{0.5cm}
	{\large or how I started worrying about high $n_e^{ped}$}\\
	{\large Univerisitat Leipzig} \\ 
	}

\author{Adam Kit \\{\small Advisors: Cichos, F. and Groth. M and Järvinen A.}}
\date{15 06 2021}
\clearrow
\begin{document}
\begin{titlepage}
   \begin{center}
       \vspace*{1cm}

       \textbf{Machine Learning Analysis on the JET Pedestal Database }

       \vspace{0.5cm}
        or how I started worrying about high $n_e^{ped}$
            
       \vspace{1.5cm}

       \textbf{Adam Kit}
       
       \vspace{0.5cm}
       \textbf{Advisors:} Cichos, F. and Groth, M and Järvinen, A.

       \vfill
            
       A thesis presented for the degree of\\
       B.Sc. Physics
            
       \vspace{0.8cm}
     
%       \includegraphics[width=0.4\textwidth]{university}
            
       Universität Leipzig\\
       Germany\\
       17.06.21
            
   \end{center}
\end{titlepage}
  %\maketitle
    \newpage
    \tableofcontents
    \newpage
\section{Introduction}\label{sec:introduction}
Harnessing controlled thermonuclear fusion on Earth is a complex multi-faceted problem; a potential solution is that of controlled magnetic confinement fusion (MCF), such as stellarators or tokamaks \cite{EUROfusionroadmap}. 
The field of MCF research is entering the era of superconducting, reactor-scale, long pulse devices, such as ITER and DEMO \cite{Ikeda_2007, stepladder}.
These reactor-scale devices encompass a significant risk of very costly component damages in off-normal events, and, hence, the emphasis on reactor and plasma scenario design is shifting from experimental approaches to theory based predict-first and plasma flight simulator methods \cite{Meneghini_2017, MOREAU2011535}.
To  bridge  the  gap  between computational and experimental efforts, and to rapidly design reactors, there exists a need fordata-driven approaches to produce models through the use of machine learning (ML). 
\begin{comment}
To allow fast throughput for flight simulation and on the fly scenario modelling, there exists a need for data-driven approaches to produce models through the use of machine learning (ML).
\end{comment}
The topic of this thesis is to analyze and compare predictive ML tools in estimating plasma parameters based on experimental data. The focus is on the plasma density in the edge of the main plasma of tokamaks, where high performance scenarios typically establish an edge transport barrier and plasma pedestal. 

\subsection{Why Pedestal Physics?}\label{subsec:purpose}
In tokamaks, the fusion plasma is confined in a toroidal vacuum chamber using magnetic fields.
The magnetic field has components around the torus, called the toroidal component, and around the cross-section of the vacuum chamber (Fig. \ref{subfig:triangularity}), called the poloidal component.
These are generated with magnetic coils and plasma currents.
The toroidal and poloidal magnetic fields generate helical field lines that are necessary to confine the plasma.
The helical field lines form nested closed flux surfaces.
At the edge of the plasma, structures of the reactor wall intersect these flux surfaces, such that they become open.
The region of the open field lines, within which the plasma is in contact with the reactor components, is called the scrape-off layer (SOL).
The last flux surface that is closed is aptly named the last closed flux surface (LCFS), and the field line that separates LCFS from SOL is called the seperatrix.
In present-day plasma scenarios, the edge plasma is magnetically diverted to a separate divertor area, such that the LCFS is not in direct contact with the wall.

Nuclear fusion with net energy gain requires sufficiently high fuel pressure and confinement time, i.e., the triple product of the density, temperature and confinement time, $nT\tau_E$,  must be high enough \cite{Lawson_1957}. A deuterium-tritium plasma is considered ignited when the heating provided by the resulting 3.5 MeV alpha particles is sufficient to overcome the energy losses. 
The highest achieved triple product in MCF devices thus far is $1.53\times 10^{21}\text{ keVs/m}^3$ in the JT-60 U tokamak operating with deuterium plasmas \cite{JT60_triple}. ITER and DEMO plan to achieve triple product values on the order of $10^{22}\text{ keVs/m}^3$ operating with deuterium-tritium plasmas \cite{EUROfusionroadmap}.

The maximum pressures achievable in MCF devices are limited by magnetic field strength and magnetohydrodynamic (MHD) instabilities.
The field strength is limited by the capabilities of the available superconductor technology such that, in conventional tokamak reactor designs, the fields in the center of the plasmas are around 5-6 T \cite{zohm_use_2019}. A useful figure of merit for an MCF device is the ratio of the confined fuel pressure to that of the magnetic field pressure, $\beta = p / (B/2\mu_0)$, which in tokamaks is always significantly less than 1 for stable plasmas. Higher $\beta$ plasmas are desired for high fusion performance and power density, but the maximum $\beta$ is limited by MHD stability. Often $\beta$ is further normalized with the Troyon factor $(B_T \, a/ I_P)$, originating from the Troyon $\beta$ limit \cite{Troyon_1988}. This is called $\beta_N$. Typically in tokamaks the confined fuel pressure is a few times the atmospheric pressure, and the energy confinement time is around seconds~\cite{FEDERICI2014882}.


The energy confinement time is limited by plasma turbulence, which leads to radial transport across the flux surfaces significantly faster than would be expected based on classical or neo-classical transport \cite{Ikeda_2007}.
Typically, the turbulence modes show critical gradient behavior \cite{Ikeda_2007}, such that the radial gradients are limited near their critical value, and the effective transport increases with more heating power.
This enhanced transport results in reduction of $\tau_E$ with heating power, as can be seen in common confinement time scalings, for low (L-mode) and high confinement mode (H-mode) \cite{Ikeda_2007}. As a result, reaching $nT\tau_E$ is challenging considering the solution of throwing power at the problem has the opposite than desired effect.

In the 1980s, a sudden transition into an enhanced confinement regime, called the high confinement mode (H-mode), was discovered in plasmas operating with the divertor configurations and neutral beam heating \cite{PhysRevLett}.
The H-mode confinement can break away from the stiff gradients at the edge, as self-organized shear flows at the plasma edge reduce turbulent radial fluxes, leading to the formation of a plasma pressure 'pedestal'.
To achieve H-mode confinement, a minimum amount of power flow through the edge is required \cite{Martin_2008}. H-mode ultimately provides about a factor of two increase of the energy confinement time. Therefore, H-mode is the standard operational mode expected in future reactor-scale devices such as ITER and DEMO.

The suppressed turbulence in the pedestal region allows the radial pressure  and current gradients to grow until they trigger MHD instabilities, called edge localized modes (ELMs) \cite{ELM_s, Viezzer_2018}.
The current understanding is that high performance pedestal plasmas are limited by ideal MHD peeling-ballooning instabilities that trigger type-I ELMs \cite{ELM_s}.
Pedestal plasmas are not always limited by ideal MHD peeling-ballooning instabilities.
For example, a large fraction of JET H-mode plasmas operating with the ITER-like wall, including beryllium main chamber and tungsten divertor targets~\cite{PHILIPPS20101581}, do not reach the ideal MHD peeling-ballooning stability threshold \cite{Frassinetti_2020}.  Therefore, determining which transport or instability phenomena limit the pedestal is a very active topic of research~\cite{electron_transport, Catto_2013, Kotschenreuther_2019}.

Since edge transport barriers are key ingredients of future ITER and DEMO burning plasma scenarios, predictive capability is necessary for the pedestal region to confidently design the future reactors and scenarios.
Due to complexity of the pedestal plasmas and the non-linear interaction of the pedestal with the core and SOL plasmas, the simulation codes for the pedestal make simplifying assumptions. The most widely used model for pedestal pressure predictions is EPED \cite{EPED_ELM, Snyder_2011}. 
EPED has been very successful for predicting pedestal pressure width and height for many present-day tokamaks, by assuming that the pressure gradient in the pedestal is limited by local kinetic balooning modes (KBM) and the total pressure by ideal-MHD peeling-balooning modes. However, these assumptions are not justified for a large fraction of the JET pedestal database \cite{Frassinetti_2020}. EPED also takes as input certain plasma characteristics, including information about pedestal densities and density profiles, confined normalized pressure, and dilution of the plasmas by impurities. Therefore, EPED cannot be considered a fully predictive model. EUROPED aims to bridge some of these shortcomings of EPED by using other models for core transport and pedestal density \cite{Saarelma_2017, pedestal_prediction}. 

In this study, the focus is predicting the pedestal density, and ML tools analyzed are compared to an experimental log-linear fit published in \cite{Frassinetti_2020}.

\subsection{JET Pedestal Database}
\begin{figure}
	\centering
	\begin{subfigure}{0.35\linewidth}
		\centering
		\includegraphics[scale=0.34]{./src/traingularity_crop_2}
		\caption{}
		\label{subfig:triangularity}
	\end{subfigure}\hfill
	\begin{subfigure}{0.60\linewidth}
		\centering
		\includegraphics[scale=0.22]{./src/MTANH_fit_21}
		\caption{}
		\label{subfig:mtanhfit}
	\end{subfigure}\hfill
	\caption{ \textbf{(a)} Plasma shape parameters from the cross-section of a tokamak. \textbf{(b)} HRTS measurement profiles (blue) radially shifted to have $T_e \approx 100$ eV at the seperatrix (LCFS). The profiles are fitted in real space using the mtanh equation (eq. \ref{eq:mtanh}) then mapped to the normalized poloidal flux coordinate $\Psi_N$ (red).}
	\label{fig:pedestal_db_figs}
\end{figure}

The JET pedestal database contains over 3000 entries, with each entry corresponding to time averaged measurements of various plasma parameters over the course of 70-95\% of an ELM cycle, representing the conditions of the highest pedestal plasma pressure prior to the next ELM.
The measurements are done using high resolution Thomson scattering (HRTS)\cite{Pasqualotto_2004}, and are then fitted using the modified tangent hyperbolic function (mtanh): 
\begin{equation} \label{eq:mtanh}
\text{mtanh}(r) = \frac{h_1 - h_0}{2} \left( \frac{(1 + sx) e^x - e^{-x}}{e^x + e^{-x}} + 1\right) + h_0 , \quad \quad x=\frac{p-r}{w/2}
\end{equation}
where the pedestal height, position, width, and slope inside the pedestal top are $h_1$, $p$, $w$, and $s$ respectively, and $r$ the normalized radius $\Psi_N$ (Fig \ref{subfig:mtanhfit}). 
Since the measurements are taken near the end of the ELM cycle, the pedestal parameters should be saturated near their maximum, right before the ELM.

The key engineering quantities in the database and their units ([-] dimensionless) are listed below:

\begin{wraptable}{r}{0.4\textwidth}
\centering
\caption{Main engineering parameter domains of the filtered dataset.}
\label{tab:main_domain}
\begin{tabular}{| c | c | }
	\hline
	Eng. Param & Domain \\
	\hline
	$I_P$ [MA] & $[0.81, 4.48]$ \\
	$B_T$ [MW] & $[0.97, 3.68]$ \\
	$a$ [m] & $[0.83, 0.97]$ \\
	$\delta$ [-] & $[0.16, 0.48]$ \\
	$M_{eff}$ [-] & $[1.88, 2.18]$ \\
	$P_{NBI}$ [MW] & $[10^{-3}, 32.34]$ \\
	$P_{ICRH}$ [MW] & $[0, 7.96]$ \\
	$P_{TOT}$ [MW] & $[3.4, 38.22]$ \\
	$V_P$ [m$^3$] &  $[58.3, 82.19]$ \\
	$q_{95}$ [-] & $[2.42, 6.04]$ \\
	$\Gamma$ [$10^{22}$ e/s] & $[0, 15.5]$ \\
	$H$ [-] & $[0, 0.18]$ \\
	$P_{SD}$ [$10^6$nbar] & $[0,1000]$ \\
	\hline
\end{tabular}
\end{wraptable}
\begin{itemize}
	\item $I_P$ [MA], plasma current, current driven through the plasma that generates the poloidal magnetic field, (Fig. \ref{subfig:triangularity})
	\item $B_T$ [T], toroidal magnetic field, (Fig. \ref{subfig:triangularity})
	\item $R$ [m], major radius of the plasma, (Fig. \ref{subfig:triangularity})
	\item $a$ [m], minor radius of plasma, (Fig. \ref{subfig:triangularity})
	\item $\delta$ [-], triangularity, normalized horizontal displacement of the top/bottom of the plasma from the main axis, (Fig. \ref{subfig:triangularity})
	\item $V_P$ [m$^3$], the plasma volume,
	\item $H$, isotope ratio of fuel,
	\item $q_{95}$ [-], safety factor at the 95\% flux surface. Safety factor is the 'windiness' of the magnetic fields in a reactor, i.e., the  number of toroidal circles the helical field line completes within one poloidal revolution. $q$ is called the safety factor as low $q$ plasmas are susceptible to MHD instabilities. Typically, the baseline scenarios operate  $q_{95}$ around 3-4,
	\item $P_{NBI}$ [MW], neutral beam injection heating power,
	\item $P_{ICRH}$ [MW], ion cyclotron radio frequency heating, 
	\item $P_{TOT}$ [MW], total power ($P_{TOT} = P_{NBI}+ P_{ICRH} + P_{OHM} - P_{SH}$, where $P_{OHM}$ is the ohmic heating due to the plasma current, and $P_{SH}$ is the power lost due to the shine through of NBI heating),
	\item $\Gamma$ [ $10^{22}$ electrons per second], gas fueling rate of the deuterium or hydrogen,  
	\item $DC$, the divertor configuration, can take on values of C/C, V/H, V/C, V/V, C/V, C/H, (see \cite{Frassinetti_2020} for more information),
	\item $TW$, the type of wall, as JET was upgraded in the mid 2010s, and moved from having a Carbon wall to an 'ITER like wall' (ILW),
	\item $P_{SD}$ [$10^6$ nbar], the subdivertor pressure.
\end{itemize}


For the main engineering parameters, the uncertainties are calculated by taking the standard deviation of the values over the time period in which the measurements were taken. 

There are also global parameters stored in the database, such as $\beta_\theta^{ped}$, $\beta_N$, $Z_{eff}$. However, in this study, we are first considering a model that only utilized machine engineering parameters as input. This assumption is in contrast to EPED that also takes plasma parameters as input, including pedestal density and core  $\beta$ for example. These type of models assume that these plasma parameters can be achieved with proper actuation of the engineering inputs. 

\begin{comment}
The global parameters stored in the database are listed below: 
\begin{itemize}
	\item $\beta_\theta^{ped}$ [-], $\beta$ is the ratio of plasma pressure $p$ to the pressure exerted by the magnetic field $B$, $\beta = p / B^2 / 2\mu_0$, thus $\beta_\theta^{ped}$ is the pressure due to the poloidal magnetic field $B_\theta$ and plasma electron pressure at the pedestal $p_e^{ped}$
	\item $\beta_N$, normalized $\beta$ for comparison between reactors, as $\beta$ has an inherent limit based on MHD stability, and is a function of the plasma current, minor radius and magnetic field such that $\beta_N = \beta / I / aB$, is commonly known as the Troyon factor
	\item $Z_{eff}$, the effective charge state of the plasma
\end{itemize}

The global parameters are certainly interesting, but within the context of this thesis are not considered to be viable inputs to a predictor, as they rely on information that is unavailable as a control knob on a reactor.
A truly predictive model cannot take plasma parameters as inputs. Today, EPED takes $\beta$, $n_e^{core}$ and $Z_{eff}$ as inputs assuming the feedback can be used to choose the density and $\beta$.
Models like EPED rely on the principle that reactor operators would 'know' these density and beta points are within reachable operational space, and that furthermore they know the recipe to get there.
\end{comment}

A model of interest is one that uses the main engineering parameters to calculate pedestal profile parameters like height, width, or position for the pedestal quantities temperature, density, or pressure.
The pedestal profile parameters stored in the database are determined using the mtanh fit eq.\;(\ref{eq:mtanh}), and the uncertainties are the fit uncertainties from the use of the mtanh function \cite{Frassinetti_2020}.
The fit uncertainties are expected to be significantly smaller than the natural scatter of the data due to the fluctuation of the plasma.

Additionally, the database entries contain the parameter FLAGS, which correspond to the specific setup of an experiment.
Examples of FLAGS contained in the database: main fueling element, usage of RMPs, pellets, or impurity seeding, divertor configuration. 
\begin{comment}
For example, what element the fuel for a shot is, or if resonant magnetic pulses (RMPs) [\textcolor{blue}{source}], pellets [\textcolor{blue}{source}] or impurity seeding [\textcolor{blue}{source}] were used in a shot are all FLAGS contained in the database.
\end{comment}
Each entry in the database is validated either by hand or computationally, and there is a FLAG corresponding to the quality of the HRTS measurement determined by the validation \cite{Frassinetti_2020}.
Only entries that have been validated are used in this thesis.
Shots with impurity seeding are used, as they make up about 600 entries.
To keep the dataset simple, entries with RMPs, pellets, and kicks are excluded, as these are used to manipulate the pedestal for ELM control, mitigation, or suppression \cite{Viezzer_2018}.

After filtering out the shots with RMPs, kicks, pellets, non-validated HRTS, and shots that do not use deuterium, the dataset is reduced to 1888 entries. The final main engineering and pedestal parameter domains are given in Tables \ref{tab:main_domain} and \ref{tab:ped_quant} respectively.
\begin{center}
\begin{table}[h]
\begin{tabular}{ | c | c | c | c | c | }
	\hline 
	& Height & Width $[\Psi_N]$ & Position $[\Psi_N]$ & Slope [-] \\ 
	\hline
	$n_e^{ped}$ &[1.849, 11.737] ($10^{19}$ m$^3$) & [0.015, 0.173]& [0.953, 1.029] & [$10^{-6}$, 0.188] \\
	$T_e^{ped}$ & [0.149, 1.894] (keV)& [0.013, 0.105] & [0.926, 1.002] & [0.026, 0.502] \\
	$p_e^{ped}$ & [0.808, 17.804] (kPa)& [0.014, 0.099] & [0.931, 1.002]& [0.041, 0.789] \\
	\hline
\end{tabular}
\caption{Domains of pedestal parameters for deuterium shots stored in the JET pedestal database after RMPs, kicks, pellets, and non-validated HRTS are filtered out.}
\label{tab:ped_quant}
\end{table}
\end{center}


\subsubsection{Empirical Analysis}

Empirical analysis of the JET pedestal database was carried out \cite{Frassinetti_2020}, and has yielded a log-linear scaling law for the pedestal density height $n_e^{ped}$:
\begin{equation} \label{eq:scaling}
	n_e^{ped} = (9.9 \pm 0.3) I_p^{1.24 \pm 0.19} P_{TOT}^{-0.34 \pm 0.11} \delta^{0.62 \pm 0.14} \Gamma^{ 0.08 \pm 0.04} M_{eff}^{0.2 \pm 0.2}
\end{equation}
In this thesis, we will focus on the pedestal prediction with ML approaches relative to the performance of the empirical scaling law.
The choice of parameters in the log-linear regression by Lorenzo Frassinetti et. al. \cite{Frassinetti_2020}, was backed by physical intuition on the pedestal density height. Since log-linear regression was used, the scaling law above avoided using cross-correlated variables, which can verified in Figure \ref{fig:emperical}. 

\begin{figure}[h]
        \centering
        \begin{subfigure}{0.5\linewidth}
                \centering
                \includegraphics[scale=0.2]{./src/R_vs_NEPED_matplotlib}
                \caption{}
                \label{subfig:icecream}
        \end{subfigure} \hfill
        \begin{subfigure}{0.45\linewidth}
                \centering
                \includegraphics[scale=0.2]{./src/input_correlations}
                \caption{}
                \label{subfig:corr}
        \end{subfigure}
	\caption{Empirical data plots of the JET pedestal database: \textbf{(a)} correlation without causation between the major radius $R$ and $n_e^{ped}$,  \textbf{(b)}  Correlation matrix of the main engineering parameters. A grey coloring represents no correlation, whereas blue and yellow are negative and positive correlation, respectively.}
	\label{fig:emperical}
\end{figure}

To improve prediction quality, it is useful to include additional inputs from the list of main engineering quantities that were not used in the log-linear scaling.
However, by plotting joint histograms between the control parameters and $n_e^{ped}$, serious questions can be raised regarding which parameters can and should be given to a machine learning model.
For example, looking at the dependence of the major radius $R$, one could jump to an early conclusion and say that with higher values of $R$, a higher pedestal height is achieved (Fig. \ref{fig:emperical}a)!
However, this is a case of causation without correlation, or ice-cream correlation\footnote{Ice-cream correlation refers to the correlation of increasing ice cream sales and increasing number of drownings in Finland during the summer. Although the variables are indeed correlated, higher ice cream sales are not in fact the cause of higher drowning rates, nor vice-versa.}, and the actual culprit of the causation is the Shafranov shift; the outward radial displacement of the magnetic axis from the geometric axis that is prominently found in MCF devices~\cite{shafranov_equilibrium_1963, freidberg_plasma_2007}. The shift increases with normalized pressure, which increases $R$. As normalized pressure increases with pedestal pressure, there is a cross-correlation between pedestal pressure and $R$. For this reason, $R$ is excluded from the list of inputs to the ML models in this thesis, and only when multi-machine databases with significantly larger variation of $R$ are available should it be included.
The divertor configuration on the other hand, does have a real correlation, and can have a large impact on the pedestal. However, the analysis in this thesis only makes use of the numerical parameters available, and thus divertor configuration will not be used as an input parameter, as it is categorical.  

Another engineering parameter that is ignored in the analysis is the sub-divertor pressure $P_{SD}$, which is correlated with the gas fueling rate, but also depends on other main engineering parameters, such as divertor configuration, input power, and wall conditions.
From the filtered dataset, the values of $P_{SD}$ vary widely, with 130 entries having an error and value of 1000 [$10^6$ nbar], while having close to 500 entries that vary between [0, 0.5].
Because of this volatility, $P_{SD}$ is ignored, however future work may choose to filter the dataset such that inclusion of $P_{SD}$ is possible.



\section{General Machine Learning Analysis}\label{sec:principle-machine-learning-analysis}
Within the context of this thesis, a model refers to a prediction function $f$ that takes any combination of the main engineering parameters as inputs, $\vec{x} = (x_1, x_2, \cdots, x_p)$,  and provides an estimate of the pedestal density height, $\hat{y}$, as well as the uncertainty of the estimate (when applicable).
The prediction quality is quantified through both the root mean squared error (RMSE) and mean absolute error (MAE), since a robust model minimizes both of these.
The RMSE score penalizes predictions that are far away from the ground truth, whereas the MAE uniformly calculates the distance between predictions and the ground truth.
\begin{comment}
\[RMSE = \sqrt{\frac{\sum_i^N \left( y_i - \hat{y}_i \right)^2}{N}} \quad\quad MAE = \frac{\sum_i^N |y_i -  \hat{y}_i |}{N}  \] 
where $N$ is the number of points predicted upon, and $y_i$ is the ground truth value of $n_e^{ped}$ for the $i$'th entry in the dataset. 
\end{comment}
\subsection{Model Fitting and Validation}
To make a prediction, the model must first be \textit{fitted}, which means to learn the parameters $\vec{\theta}$ of the prediction function via a model-specific learning algorithm such that $f = f(\vec{x}: \vec{\theta})$.
In the case of linear regression, the learning algorithm is the ordinary least squares method (OLS), which minimizes the mean squared error in order to find the optimal linear coefficients $\theta(w_i)$ \cite{OLS}.
Not all supervised learning regression algorithms minimize the RMSE or MAE to fit model parameters. 

The performance of the fitted model is scored using data that the model has not observed previously.
If this were not the case, the model would simply repeat predictions that  it had been fitted with, and would fail to predict useful information on  unseen data.
This is called \textit{overfitting}. To avoid overfitting a common practice in supervised machine learning is to hold part of the available data as a test set.
This can be done by randomly splitting the available data into training and test subsets and by evaluating the model on the test set.
Depending on the performance on the test set, the \textit{hyperparameters} of the model are adjusted to optimize the performance on the test set  (an example of a hyperparameter is the number of trees in a random forest, or learning rate for artificial neural networks).
However, this method runs into the problem of overfitting on the test set, since the hyperparameters can be adjusted until the model performs best on the test set.
In this sense, knowledge about the test set leaks into the model, and evaluation metrics no longer report on general performance.
Additionally, in randomly splitting the data into two groups, there is a new problem of \textit{selection bias}, in which the results are dependent on the random choice of entries contained in the training and test sets \cite{Selection_Bias}.
To overcome these problems, \textit{cross-validation}, or CV, is implemented throughout the analyses in this thesis to validate the parameters and generalization capabilities of a model.
The general approach for \textit{k-fold} CV is to split the dataset into $k$ subsets, and apply the following procedure to each of the k folds:
\begin{itemize}
	\item Use $k-1$ of the folds to fit a model 
	\item Hold out the remaining fold to validate the fitted model
\end{itemize}
Furthermore, \textit{repeated k-fold} CV is employed, in which the above process is repeated $p$ times.
The final performance measure is then the average of the scores on the test sets left out.
This method is very computationally expensive since $k*p$ models are being fit, but it is extremely efficient with the data, while additionally removing selection biases with sufficient folds and repeats.

\subsection{Linear Regression}
A commonly used approach in the plasma physics community is log-linear regression to create scaling laws like that from Lorenzo Frassinetti et. al \cite{Frassinetti_2020}.
A general overview of linear regression can be found in ~\cite{hastie01statisticallearning}. 
Additional details that are used in this thesis are as follows: 
\begin{itemize}
	\item By minimizing the MSE through OLS, the scalar linear coefficients $w_i$ corresponding to the control parameter $x_i$ can be determined, and from these coefficients we learn the linear correlation of an engineering parameter and $n_e^{ped}$, i.e., if the coefficient in front of the plasma current, $w_{I_P}$, is positive, then as $I_P$ increases, so will the prediction of $n_e^{ped}$.
	\item By adding a regularization term ($L^1$ norm of the weights) to the MSE cost function, the coefficients will be minimized as well, resulting in some coefficients becoming zero. From this procedure it is determined whether an engineering parameter is 'useful' in the context of predicting point estimates of $n_e^{ped}$ using linear regressors, and can reduce dimensionality when possible. This is known as LASSO. \cite{Bisong2019, LASSO_OG, LASSO_COIN}
	\item By transforming the point prediction of $n_e^{ped}$  into a normal distribution we can obtain the uncertainty in the prediction. This is done by determining the optimal linear coefficients via maximum likelihood estimation instead of OLS, and thus the coefficients transform from scalars into normal distributions with mean $\mu$ centered around the scalar coefficient and spread $\sigma^2$ representing the uncertainty in the coefficient. This is otherwise known as Bayesian Regression~\cite{bayes_regr}.
\end{itemize}

\begin{comment}
The uncertainty in the prediction can be determined by transforming the weights from scalars into normal distributions with mean $\mu$ centered around the scalar coefficient, and spread $\sigma$ representing  the  uncertainty  in  the  coefficient,  thus  transforming  the  point estimate into a distributional estimate.  This is otherwise known as Bayesian Regression[SOURCE]
\end{comment}
By using more input parameters than those which are used in the scaling law, eq. (\ref{eq:scaling}), the intent is to achieve a better (lower) RMSE while additionally maintaining interpretability, i.e., attach physical intuition behind why the determined coefficients are the way that they are. However, by including more parameters, we also introduce cross-correlation to the system, and expect that the coefficients determined may vary from those in the scaling law. We do not expect any new revelations from the coefficients determined by a linear regressor using more engineering parameters than those listed in Eq. (\ref{eq:scaling}), i.e., the pedestal density will still increase as the plasma current increases.  The linear models analyzed come from the sklearn and PyMC libraries \cite{scikit-learn, Salvatier2016}. 

\subsection{Gaussian Processes}
In contrast to linear models, Gaussian Processes (GPs) are non-parametric, in that there is no function to be minimized, but rather an optimal set of functions are found that best characterize predicting $n_e^{ped}$ given the engineering parameters as inputs.
Much more can be read about GPs in the following sources \cite{gortler2019a, Rasmussen2004, vapnik95}.
The details pertinent to the analysis in this thesis are as follows:
\begin{itemize}
	\item Choice of kernel (covariance function) is normally based on the 'wigglieness' of the functions one is trying to parameterize \cite{cov-matrix-maths}, but with higher dimensional space, this means nothing. Therefore, the kernels desired are those that best predict the pedestal density when optimized.
	\item Prediction uncertainty is built into GPs as the joint-Gaussian group of functions determined through optimization of the kernel will give predictions of the pedestal density that are averaged for the point prediction, and the standard deviation is the uncertainty \cite{Rasmussen2004}. 
	\item Sensitivity analysis is used to determine the relevant engineering parameters for GPs, from which the dimensionality of the input space can be reduced if parameters are deemed irrelevant. Three different forms of sensitivity analysis are used and are described below \cite{pmlr-v89-paananen19a}. 
	\item There are two approaches to utilize the measurement uncertainties given in the database: (a) a fixed noise kernel is added on to the base kernel such that the measurement uncertainties are additive to the input space, (b) transforming the GP model from homoscedastic to heteroscedastic, where the homoscedastic model assumes constant Gaussian noise and the heteroscedastic takes noise values that vary for each input entry. Furthermore, the heteroscedastic model attempts to learn the uncertainty space given uncertainty of the input, i.e., both the mean and \textit{local} variance of $n_e^{ped}$ are estimated~\cite{heteroscedastic}.   
\end{itemize}

GPs scale unfavorably with increasing input space size, therefore by using sensitivity analysis, the intent is to remove engineering inputs if they do not improve the prediction capability of the GP model.

Three types of sensitivity analysis are used:
\begin{itemize}
	\item Automatic Relevance Determination (ARD): The predictive relevance of each input variable is inferred from the inverse of the length-scale parameter associated with that variable within the kernel. A large length scale (infinite, for example) means that there is no correlation between the latent space and the variable in question, and thus the relevance would be zero \cite{pmlr-v89-paananen19a}. 
	\item Kullback-Leibler Divergence (KLD): The KLD is a well known measure of dissimilarity between two probability distributions, and is a function of both the latent mean and uncertainty of each distribution \cite{KLD}. In this case, the input space is 'shifted' via perturbing values of individual variables, and the KLD of the resulting new latent space is measured against the unperturbed case. A large change in the KLD indicates that the single variable that was perturbed has high prediction relevance \cite{pmlr-v89-paananen19a}. 
	\item Variance of the Posterior (VAR): The same method of perturbation applies, but instead of calculating the KLD, variability in  only the latent mean of the fitted GP is calculated \cite{pmlr-v89-paananen19a}.
\end{itemize}

The Gaussian Process models used in this analysis are adapted from the GPy library \cite{gpy2014}. 

\subsection{Random Forests}
Another popular non-linear model is the ensemble of decision trees \cite{DT_OG} that is the Random Forest(RF). More can be read about Random Forests here \cite{RF_OG, 598994}, but the details pertinent to the thesis are stated below:
\begin{itemize}
	\item RFs are fitted using bootstrap aggregation (bagging). Each decision tree within the forest is fit from a 'bag' of random samples drawn from available training entries, meaning not every tree will see every available training sample, allowing for the calculation of the average error for each sample using the predictions of trees that do not contain the sample in their bag. This allows us to approximate how many decision trees to use in the forest, as the OOB error will eventually stabilize. The bag consists of a predetermined number of features which are also randomly sampled, which allows for the determination of the optimal number of inputs to sample, as well as which inputs are optimal ~\cite{hastie01statisticallearning}.
	\item UQ in prediction can be determined by taking the standard deviation of predictions from all of the decision trees that make up the forest. 
	\item A variant of RFs called Extremely Randomized Trees (ERTs) will also be compared. The two main differences between RFs and ERTs are (a) decision trees in ERTs sample entries for their bags without replacing them such that no decision tree contains any of the same entries and (b) nodes in decision trees are split based on different criteria; RFs convert parent nodes into two homogeneous nodes by choosing the split that minimizes the MSE, whereas ERTs convert the parent node into two child nodes via a random split~\cite{geurts_extremely_2006}.
\end{itemize}

Random Forests and Extremely Randomized Trees offer little interpretability in comparison to parametric models, but by quantifying how much the impurity of a node decreases (a pure node has no child nodes) with the probability of reaching that node, the relative importance of the feature housed in the node is determined. Using this, we can get insight into which features are driving the predictions of $n_e^{ped}$ for RFs and ERTs. The RFs and ERTs used in this analysis are adapted from the sci-kit learn library \cite{scikit-learn}. 

\subsection{Artificial Neural Networks}

Numerous previous studies have investigated the reasons why artificial neural networks (ANNs) work \cite{silver_mastering_2016, 7333916, CALTECH}.  The ANNs used in this thesis are all fully-connected feed-forward networks \cite{Goodfellow-et-al-2016, SCHMIDHUBER201585}. Work has already started on using feed-forward ANNs in predicting pedestal quantities \cite{Andreas}. 
Since ANNs are very delicate, the primary goal in this thesis is to probe the hyperparameter and architecture spaces for future research upon which to build. We optimize the following hyperparameters: mini-batch size, learning rate, number and size of hidden layers, activation functions, layer types, length of training, and regularization. The optimal hyperparameters are those that achieve the lowest RMSE through the cross-validation process.

In this thesis, the prediction uncertainty of ANNs is determined through grouping many ANNs of similar hyperparameters into an ensemble such that the standard deviation of the predictions of each network in the ensemble is the uncertainty in the ensemble prediction.

The ANNs used in this analysis are built using PyTorch~\cite{NEURIPS2019_9015}. 


\subsection{Meta-Modeling}
Another use of the uncertainties stored within the database is for a given entry to generate normal distributions with the mean of a parameter value and spread of the uncertainty. By sampling from the generated distribution, it is possible to create new 'synthesized' entries.
As seen in the next section, many models can predict well for $n_e^{ped} \leq 10$, but struggle for densities higher than that.
By including synthesized values in the fitting procedure, the hope is that models would be able to better predict higher $n_e^{ped}$ values.
\vspace{3cm}

\section{Results}{\label{sec:results_1}
Each model analyzed is fit using the following list of main engineering parameters as inputs:
\begin{itemize}
        \item $I_p, \; B_T,\; a, \; \delta, \; M_{eff},\; P_{NBI},\; P_{ICHR},\; P_{TOT}, \; q_{95},\; \Gamma, \; H,\; V_P$.
\end{itemize}
There are cross-correlations among these parameters (Fig. \ref{fig:emperical}), and, as will be observed, some models do not perform very well due to these correlations. 
Through the use of cross-validation, the hyperparameters of each model are tuned such that optimal performance is achieved on the average performance on each fold subset.
The relevant hyperparameters, and how they were determined, is discussed in each individual model subsection, as well as the effect of meta-modeling when applicable.
Table \ref{tab:performance_models} presents the final performance metrics for each optimized model. 

\begin{table}[h]
	\begin{center}
		\begin{tabular}{>{\rowmac}c|>{\rowmac}c|>{\rowmac}c<{\clearrow}}
		\hline
		Model & RMSE & MAE \\
		\hline
		Scaling Law & $0.9203 \pm 0.63$ & $0.7189 \pm 0.63$ \\
		\hline
		Linear & $0.8166 \pm 0.0605 $ & $0.5956 \pm 0.0379$ \\
		\setrow{\bfseries}GP & 0.4566 $\mathbf{\pm}$  0.0217 &  0.3395 $\mathbf{\pm}$ 0.01383\\
		RF & $0.5938 \pm 0.0352$ & $0.4225 \pm 0.0191$ \\
		ERT &$0.5623 \pm 0.0368 $ & $0.3927 \pm 0.0199$ \\
		ANN & $0.6126 \pm 0.0694$ & $0.4418 \pm 0.0421$ \\
		\hline
		\end{tabular}
		\caption{The best RMSE and MAE of the predictions from each optimal model are calculated by averaging the results across each fold and repeat of the repeated cross-validation method. Uncertainty in the calculated RMSE and MAE is derived from the  standard deviation of the RMSE across each fold.}
		\label{tab:performance_models}
	\end{center}
	\vspace{-38pt}
\end{table}

\subsection{Linear Regression}

Using a Bayesian linear regression model without an intercept using the control parameters as inputs, the RMSE and MAE improve by including more parameters than in the reference scaling law, eq. (\ref{eq:scaling}) (Fig. \ref{fig:lin_reg}).
\begin{figure}[hb!]
	\centering
	\begin{subfigure}{0.49\linewidth}
		\centering
		\includegraphics[width=6.5cm]{./src/Bay}
		\caption{}
		\label{subfig:bayes_pred}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\centering
		\includegraphics[width=6.5cm]{./src/linear_comp_v3}
				\caption{}
		\label{subfig:bayes_resid}
	\end{subfigure}
	\caption{Comparison of a Bayesian Ridge regressor fit using all available engineering parameters against the log-linear scaling law eq.\;( \ref{eq:scaling}). \textbf{(a)} The predictions of a Bayesian regressor (green) vs the scaling law (orange) with the ground truth being the black dotted line. \textbf{(b)} The Residual comparison of the Bayesian regressor (green) and the scaling law (orange) }
	\label{fig:lin_reg}
\end{figure}

Due to cross-correlated variables, we observed that LASSO did not perform well in reducing the dimensionality of the input space. The LASSO method deemed the following features as 'unimportant' (their coefficients dropped to zero or near zero): $H, M_{eff}, B_T, V_P$. A new model with reduced dimensionality was fit by removing the unimportant features from the input space and the prediction quality decreased dramatically with any reduction of dimensionality; in removing H or $M_{eff}$ from the input space, the RMSE and MAE increased by 0.3, while when removing $V_P$ or $B_T$, there was an increase of 1.5 in the RMSE and MAE. 
The results of LASSO can be slightly misleading, as linear regressors and their regularized extensions are prone to problems when working with many correlated variables and many of the variables in the input space are correlated (e.g., $P_{TOT}, P_{NBI}, P_{ICRH}$), which may be why the best RMSE and MAE are achieved when all variables are included.


%------------------------------------------
\begin{wraptable}{r}{0.36\linewidth}
\centering
\caption{Coefficients determined by Bayesian Linear Regression. Each coefficient is a normal distribution with mean $\mu$ and spread $\sigma^2$}\label{tab:new_coef}
\begin{tabular}{ | c | c | c |}
			\hline
			Feature & $\mu$ & $\sigma^2$ \\
			\hline
			$I_p$ & 0.15 & 0.06 \\
			$B_T$ & 0.956 & 0.072 \\
			$a$ & 2.966 & 0.479 \\
			$\delta$ & 12.95 & 0.154 \\
			$V_P$ & -0.05 & 0.007 \\
			$q_{95}$ & -1.064 & 0.0542 \\
			$P_{NBI}$ & $-1.911$ & 0.0546 \\
			$P_{ICRH}$ & -1.976 & 0.0561 \\
			$P_{TOT}$ & 1.926 & 0.0557 \\
			$\Gamma$ & 0.125 & 0.007 \\
			$H$ & -4.016 & 0.374  \\
			$M_{eff}$ & 1.369 & 0.053 \\
			\hline
\end{tabular}
\end{wraptable}
%------------------------------------------
Fitting a Bayesian Ridge regressor using all of the available engineering parameters yielded new coefficients (as well as the uncertainties) (Table \ref{tab:new_coef}). It is observed that the coefficients for $P_{TOT}$ compete with $P_{NBI} \text{ and }  P_{ICRH}$. 
From Table \ref{tab:main_domain} we know that there are plasmas with $P_{NBI}$ 100\% of $P_{TOT}$, so the regressor balances the two out by choosing $P_{TOT}$ and $P_{NBI}$ to have opposite signs.     
From the coefficients and their uncertainties, the general uncertainty in the point prediction of $n_e^{ped}$ can be ascertained. The uncertainties were normally distributed and range from 1.64 to 1.8 ($10^{19}\text{m}^{-3}$). Having high uncertainty is good when the prediction is far from the ground truth (high residual), but for predictions on $n_e^{ped} \geq 8.5$ the uncertainty no longer covers the residual. 
Just like the scaling law, the predictions from the Bayesian linear model taper off at around $n_e^{ped} \geq 8.5$, which suggests that parametric models like linear regressors are generally unable to capture the non-linear complexity of higher pedestal density heights from the given set of input parameters.


\subsection{Gaussian Process}

Sensitivity analysis of GPs yielded the following variable importance ranking (Fig. \ref{fig:GP_dim}): 
\begin{itemize}
	\item  $\delta, a, I_p, V_P, P_{NBI},\Gamma, P_{TOT},  q_{95},  P_{ICRH}, B_T, H, M_{eff}$. 
\end{itemize}
To determine these results, a GP model with a radial basis function (RBF) kernel~\cite{kernel_cookbook} with added constant bias term was fit using all main engineering parameters, and the relevance of each variable is calculated using ARD, KLD, and VAR. This process is repeated 5 times and the results over all 5 are averaged. Each method gives relatively similar results and suggest that H and $M_{eff}$ do not aid in predicting the pedestal density (Fig. \ref{fig:GP_dim}a). This may be due to the filtered dataset, which was exclusive in its values for $H$ and $M_{eff}$ (only deuterium experiments were considered). This is expected to change if a wider range of fueling elements was included in the dataset, and in future work we do not expect $M_{eff}$ to rank as low as it does. Each sensitivity analysis also ranks $B_T$ low, which is most likely due to the inherent correlation between $B_T, \; q_{95}, \text{ and } I_P$, as most of the information of $B_T$ is contained within $q_{95} \text{ and } I_P$. 
\begin{comment}
\begin{itemize}
	\item \textbf{ARD}: $\delta, a, V_P,  I_p, P_{ICRH}, \Gamma, P_{NBI},  B_T, P_{TOT}, q_{95}, H, M_{eff}$
	\item \textbf{KLD}: $\delta, a,  I_p, P_{NBI}, V_P, P_{TOT},  q_{95}, \Gamma, B_T, P_{ICRH}, H, M_{eff}$
	\item \textbf{VAR}: $\delta, a, I_p, \Gamma, V_P, P_{NBI}, q_{95}, P_{ICRH}, P_{TOT}, B_T, H, M_{eff}$
\end{itemize}
\end{comment}

\begin{figure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=8.5cm]{ ./src/GP_sensitivity_analysis_final_v1_scaled}
				\caption{}
		\label{subfig:GP_sens}
	\end{subfigure}\hfill
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=8.5cm]{ ./src/final_GP_dim_v1_scaled_words}
				\caption{}
		\label{subfig:GP_dimens}
	\end{subfigure}
	\caption{Steps towards dimensionality reduction through the use of Gaussian Processes. \textbf{(a)} The result of sensitivity analysis as well as the average of the three types used plotted in the dashed black line. \textbf{(b)} The dimensionality order of input variables comes from their ranking via the average of the three sensitivity analyses (dashed black line in diagram to the left). For each kernel, a GP model is fit using cross-validation (5 folds, 5 repeats) for each additional dimension of data, starting with 1d input of $\delta$, followed by 2D input of $\delta, a$ and so on. The RMSE is calculated on folds left out and averaged across all folds. } \label{fig:GP_dim}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=0.23]{ ./src/MLP_residual}
				\caption{}
		\label{subfig:MLPW_RES}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=0.23]{ ./src/MLP_uq}
		\caption{}
		\label{subfig:MLP_UQ}
	\end{subfigure}
	\caption{Three different methods of uncertainty quantification are compared for a GP with an MLP kernel. \textbf{(a)} The true values of $n_e^{ped}$ are grouped into 11 equal-sized bins and the distance between the predictions of those values and the true values are calculated. The residuals are averaged across each bin and compared between the methods. \textbf{(b)} The same binning procedure, but with averaging the prediction uncertainties of the same three methods are compared.}
	\label{fig:MLP_UQ}
\end{figure}


Using the results from the sensitivity analysis, it was found that $H$ and $M_{eff}$ do not aid in improving predictive quality of GPs and that the Multi-layer perceptron kernel (MLP)~\cite{gpy_MLP} and Rational Quadratic (RQ)~\cite{kernel_cookbook} kernels perform the best (Fig. \ref{fig:GP_dim}b).  We observe that the MLP and RQ do not improve after $B_T$ is added to the input space, thus confirming the results of the sensitivity analysis (Fig. \ref{fig:GP_dim}b). This means that regarding the current dataset, it is unnecessary to supply $H \text{ and } M_{eff}$ to a GP, which will reduce the computation time when fitting. However, this reduction may be subject to change, since, as stated above, the dataset used was exclusive in its choice of $H$ and $M_{eff}$; thus for databases with multiple hydrogen isotopes included, the importance of these two variables may be more than what was found here. Nevertheless, the remainder of the GP models that are analyzed in this section do not use $H$ or $M_{eff}$ during the fitting procedure. 

 

Through uncertainty propagation it was observed that a heteroscedastic model was able to map the relative measurement uncertainties into its latent posterior at the cost of decreased prediction quality (Fig.  \ref{fig:MLP_UQ}). 
To determine the effect of uncertainty propagation, the two approaches described in Section 2 are applied; (a) a fixed kernel with $n_e^{ped}$ measurement uncertainties along the diagonal is added to a base kernel, (b) a heteroscedastic model fixes the built-in noise variance component of GPs for each input entry to be the entries respective measurement uncertainty of $n_e^{ped}$ such to learn the latent space of the used uncertainty. This process is done for both the RQ and MLP kernel and compared to the homoscedastic models.
The homoscedastic slightly outperforms the heteroscedastic model (Fig. \ref{fig:MLP_UQ}). However, the uncertainty of the homoscedastic model is on average 600\% higher than that of the heteroscedastic model. As the heteroscedastic MLP model attempts to learn the uncertainty space, we can see that although the predictions for $n_e^{ped} > 10$ are furthest, its uncertainty is much lower. Since heteroscedastic GP learns the propagated uncertainties, the generated prediction functions end up being much closer to each other, resulting in a lower variance in prediction. This is very different from the homoscedastic and fixed models, which although performing very well, suffer from having their prediction-generating functions be far apart (variance $ \geq \pm 1.2$), regardless of prediction accuracy. The heteroscedastic model tends to be overconfident at higher densities. However, the uncertainties closely match that of the actual local uncertainty, which is the goal of a heteroscedastic model (Fig. \ref{fig:MLP_UQ}).

\begin{comment}
\begin{wrapfigure}{r}{0.45\linewidth}
	\caption{}	
	\centering
	\vspace{-14pt}
	\includegraphics[scale=0.2]{./src/GP_preds_impurity}
	\vspace{-20pt}
	\label{fig:GP_impure}
\end{wrapfigure}

We also compare how the GPs predict on each type of impurity as seen in Figure \ref{fig:GP_impure}. It is well known that Nitrogen (7.0) and Carbon (6.0) play a large roll in the pedestal, and therefore models could in general have the most trouble in predicting these quantities [\textcolor{blue}{SOURCE}]. On the other hand, Neon (10.0) and Argon (18.0) also play a roll on the pedestal (although for different reasons than Nitrogen and Carbon), yet the GPs are able to predict more accurately on those. Nearly three-quarters of the entries in the filtered dataset are unseeded (0.0), and of the remaining third, about 80 percent have Neon seeding, and even fewer of the rest (for example only 3 entries have Argon or Oxygen (8.0) seeding). Even with the equally few amount of entries for both Carbon, Nitrogen, Argon and Neon, the GP models are still able extrapolate the relations of Argon and Neon better than the entries with Carbon and Nitrogen. This suggests that still there exist pedestal dependencies on Carbon and Nitrogen seeding that current GP models can not extrapolate.
\end{comment}

\subsection{Random Forests}

The optimal number of decision trees and features to sample to utilize in RFs and ERTs was determined using the out-of-bag error (Fig. \ref{fig:RF_exp}). 
\begin{figure}[h!]
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=0.19]{./src/RF_oob_error}
				\caption{}
		\label{subfig:RF_oob}
	\end{subfigure}
	\begin{subfigure}{0.5\linewidth}
		\centering
		\includegraphics[scale=0.19]{./src/ERT_oob_error}
				\caption{}
		\label{subfig:ERT_oob}
	\end{subfigure}
	\caption{OOB error vs number of decision trees of \textbf{(a)} Random Forests \textbf{(b)} Extreme Random Trees. No cross validation is necessary for this procedure, since the process of bagging inherently prevents all trees from seeing the same data. The colored lines correspond to different RFs and ERTs which vary by the amount of features to sample when splitting a node.}
	\label{fig:RF_exp}
\end{figure}

We see that after 254 decision trees in the RF, the OOB error does not improve, i.e., the forest reaches its maximum generalization capability at 254 decision trees. Furthermore, the optimal number of features to sample is 5. This is completely different from what occurs with the ERTs, where the optimal number of features to sample is 12 (all), and the number of trees is 142. The larger number of features sampled by the ERT compared to the RF is most likely due to the random splitting of nodes that ERTs make use of in creating their trees, such that they need to make use of all the features in order to generalize better, whereas the RF aims to minimize the MSE with their splits, thus not requiring all the inputs for an optimal split. For both models, overfitting begins to occur as more trees are added past the minimum OOB, and although the RMSE may improve, the generalizability does not.


\begin{wrapfigure}{r}{0.5\textwidth}
		\centering
		\vspace{-18pt}
		\caption{Effect of meat-modeling in RF and ERTs. The \# of synthesized samples added into dataset is plotted against the resulting RMSE of a model fitted using the synthesized samples.}
		\includegraphics[width=\linewidth, keepaspectratio=true, trim=0 0 0 30, clip]{./src/ERT_vs_RF_meta_modeling}
		\vspace{-22pt}
		\label{fig:RF_meta_model}
\end{wrapfigure}
It is clear that the individual predictors in the random forest do not vary as much as that of the homoscedastic/fixed Gaussian Process models (Figs. \ref{fig:RF_exp} and \ref{fig:RF_Preds}).
Additionally, the uncertainties are generally around equal to that of the residual for the corresponding bin. This is good, as the model is able to more or less provide an uncertainty that covers its residual. This does not hold for $n_e^{ped} \geq 10.0$, but it is certainly within the ballpark. 

\begin{comment}
Similar to GPs, the RFs and ERTs rank $H \text{ and } M_{eff}$ as unimportant, however reducing dimensionality did not improve prediction quality. Tree methods favor features with high cardinality, therefore $H$ and $M_{eff}$ are not as favourable since the fitting dataset contains only deuterium shots. In contrast to GPs, RFs and ERTs do not favor the power variables, $P_{NBI}, P_{TOT}, P_{ICRH}$, since they are cross-correlated. 
\end{comment}


Meta-modeling had generally no effect on RFs and ERTs (Fig.~\ref{fig:RF_meta_model}).
Although only up to 500 meta-model samples are visualized in Figure~\ref{fig:RF_meta_model}, the sporadic bouncing of the RMSE between 0.57 and 0.58 repeats for when even 2000 synthesized entries are added!
This suggests that in order to minimize the MSE of predictions across the entire dataset, RFs and ERTs ignore the additional entries and we conclude that meta-modeling has little to no effect on ERTs and RFs.


\begin{figure}[h!]
		\centering
		\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=0.23]{ ./src/Tree_residual}
				\caption{}
		\label{subfig:RF_RES}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=0.32]{ ./src/Tree_uncert}
		\caption{}
		\label{subfig:RF_UQ}
	\end{subfigure}
		\caption{Comparison of predictions and uncertainties between Random Forests (green) and Extreme Random Trees (blue). \textbf{(a)} The residual between predictions and the true values,\textbf{(b)} The prediction uncertainties on the same bins of the residuals. }
	\label{fig:RF_Preds}
\end{figure}


\subsection{Artificial Neural Networks}

\begin{wrapfigure}{r}{0.5\linewidth}
	\centering
	\includegraphics[scale=0.2]{./src/ANN_performace_size}
	\caption{Comparison of the size of an ANN and its predictive capability. Each point represents an ANN which hidden layers are equal in size.}
	\label{fig:ANN_dim}
\end{wrapfigure}
Shallow ANNs ($3-5$ hidden layers) outperform larger ANNs (Fig. \ref{fig:ANN_dim}). Therefore, the hyperparameter optimization process is focused on these networks. Since the main engineering parameters vary in magnitude (scalar value of $P_{TOT}$ is much greater than that of $q_{95}$), each parameter is scaled such that it has a mean of 0 and standard deviation of 1. Throughout the hyperparameter optimization process, each model was trained and tested using the repeated cross-fold validation method, with 5 folds and 5 repeats, and the average RMSE of predictions on the left out sets is the overall performance of the ANN. The first hyperparameters to be optimized are the learning rate (LR) and mini-batch size (MBS), and using random search, the optimal MBS and LR were found to be 396 and 0.004 respectively. Considering the dataset is around 2000 entries, the MBS is relatively large, while the LR could be considered very small for such a large MBS. However, since each training epoch, the data is randomly shuffled, it is possible that the training samples in each batch 'compete' with each other's gradient. For example, the training samples of $n_e^{ped} \geq 9.5$ pull the model weights in an opposite direction than that $n_e^{ped} \leq 9.5$. The gradient updates applied by the learning rate for this mini-batch size seemed to balance the effect of the training samples, thus resulting in the best training/testing performance. 

Then, via a grid search (across all available activation layers offered by PyTorch), the optimal activation function was determined to be ELU (Exponential Linear Unit), a close cousin to the well known ReLU (Rectified Linear Unit)~\cite{pytorch_act}. Both are ridge functions that act on a linear combination of the input variables, but since they are applied element-wise (for each node in each layer), they are non-linear. Since the above tools like GPs and RFs are non-linear models, it makes sense that a non-linear activation function performs the best. 

The initial ansatz of optimal hidden layers was determined to be either 3, 4 or 5, with between 1000-2000 total nodes (split between each of the hidden layers) (Fig. \ref{fig:ANN_dim}). This criteria was used as a space for further architecture search via random search, while using the optimal MBS, LR and activation function during the search. 
The optimal sizes of each layer for 3, 4, and 5 hidden layer networks is listed below: 
\begin{itemize}
	\item \textbf{3 Hidden Layers}: 483, 415, 254
	\item \textbf{4 Hidden Layers}: 636, 537, 295, 261
	\item \textbf{5 Hidden Layers}: 390, 484, 678, 290, 284
\end{itemize} 
The network with 4 hidden layers performed the best, with an optimal RMSE of $0.6596 \pm 0.023$. 
It was generally seen that shallow networks ($\leq 4$ hidden layers) with steadily decreasing layer size performed the best, whereas the larger networks performed best with this 'bell' shaped size, as seen in the 5 hidden layer network above. 
There could very well be better sizes out there, but the general idea of a large first hidden layer followed by steadily decreasing layer sizes performs the best.

In ensembles it was observed that the prediction uncertainty grows with increasing ensemble size, while the prediction quality generally did not change (Fig. \ref{fig:ANN_UQ}). The ensembles were comprised of the top performing 3 layer ANN, each network in the ensemble having different initial weights, and the prediction and uncertainty were compared across varying ensemble sizes. As the ensemble size grows, there is a slight decrease in RMSE and thus improved prediction quality (Fig. \ref{fig:ANN_UQ}). However, this comes with the cost of higher prediction uncertainty, where the uncertainty in the ensemble with 15 ANNs has nearly double the uncertainty for that of 5 ANNs. Although the prediction uncertainty covers the spread, it does so by a margin that resembles those of the GPs, yet the ANN's increase as we reach higher pedestal densities unlike that of the GPs. 

Meta-modeling as an additional form of UQ did seem to improve predictions for high $n_e^{ped}$, while sacrificing the overall performance.
This was unique for the ANNs, as all other usages of meta-modeling for other ML tools either did not affect the model or had only adverse effects to performance.
\begin{figure}
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=7cm]{./src/ANN_residual}
				\caption{}
		\label{subfig:ANN_res}
	\end{subfigure}
	\begin{subfigure}{0.5\linewidth}
		\centering
		\includegraphics[width=7cm]{./src/ANN_uncert}
				\caption{}
		\label{subfig:ANN_uncert}
	\end{subfigure}
	\caption{A three layer feed forward fully connected ANN with layer sizes 483, 415, 254 was transformed into ensembles of sizes 5 (blue), 10 (orange, 15 (green) and compared: \textbf{(a)} The residual between predictions and the true values, \textbf{(b)} The prediction uncertainties on the same bins of the residuals. }
	\label{fig:ANN_UQ}
\end{figure}
\begin{comment}
\begin{figure}
	\begin{subfigure}{0.49\linewidth}
		\centering 
		\includegraphics[scale=0.3]{./src/ANN_1}
		\caption{}
		\label{subfig:ann_1}
	\end{subfigure}
	\begin{subfigure}{0.5\linewidth}
		\centering
		\includegraphics[scale=0.3]{./src/ENSEMBLE_5}
		\caption{}
		\label{subfig:ensemble}
	\end{subfigure}
\end{figure}
\end{comment}
\subsection{What did not work}
Many different machine learning models not listed in the above analyses were initially tested, but due to their under-performance compared to RFs/GPs/ANNs, they were abandoned before delving deeper them. For example, Nearest neighbors methods like K-nn and R-nn~\cite{Mucherino2009} were tested and abandoned. R-nn especially so, which although they could overfit extremely well with sufficiently small radius, they can not generalize for data entries which they are not fitted with, most often providing no prediction. Support vector machines (SVMs) \cite{hastie01statisticallearning} were tested, but due to the difficulties of deriving the prediction uncertainty and general under-performance, they were dropped. Additionally, SVMs work well when there is a clear separation between regression points, but as we have seen, for $n_e^{ped} \geq 9$ there is no clear cut discrepancies, so SVMs offer little to no utility. Additionally, it is difficult to obtain uncertainties in predictions when using neighborhood methods and SVMs, and one of the goals of this thesis was to analyze models that could provide uncertainties. Other ensemble methods like AdaBoost~\cite{schapire2013explaining} showed to perform equally or slightly worse than RFs and ERTs, but some scope was needed in this thesis, so they were ultimately dropped from analysis. The combination of multiple model types into an ensemble (e.g. voting ensemble of ERT, RF and GPs) proved only slightly beneficial, and may be looked into further in future research. 
\section{Conclusion and Outlook}
The pedestal is key to high performance H-mode operation. Machine learning is needed for fast predictions for plasma scenarios. In this work, application of ML tools for $n_e^{ped}$ predictions using the JET pedestal database was done. Future work should expand these studies to other pedestal parameters, such as $T_e^{ped} \text{ or } p_e^{ped}$ as well as the other features of the pedestal profile (slope at pedesetal top, position, width). Future work could also consider combining numerical and experimental data through Bayesian methods such as described in \cite{WU2018417}. 

It is clear that major improvements towards predicting the pedestal density height is achieved through the use of non-linear machine learning models while only using main engineering parameters.

The models analyzed are ranked via their respective performance on unseen data through the use of cross validation (Table \ref{tab:performance_models}).
It is hard to point to a clear winner among the non-parametric/linear models, as they all score relatively close to each other, but it is easy to see that all ML tools analyzed outperform the scaling law. Strictly speaking, the homoscedastic Gaussian Process model with an MLP kernel achieved the lowest RMSE and MAE. All models could predict well on pedestal densities less than 9.5, however they also all struggled on densities higher than that.
\begin{comment}
The non-linear models performed worse on Carbon and Nitrogen seeded entries than they did for other impurity seedings, therefore future work would include further independent analysis into why that happens.  
\end{comment}

For each model, the prediction uncertainty was ascertained. The prediction uncertainty for Random Forests, Extreme Random Trees, aptly covered the residuals of their predictions without being 'overly cautious', while the homoscedastic and fixed kernel GPs generally held relative constant with high uncertainties for each prediction. We were able to utilize the measurement uncertainties of $n_e^{ped}$ in the JET pedestal database by propagating them into a heteroscedastic Gaussian Process model, and were ultimately able to (roughly) map the latent uncertainty space. This could prove useful should these machine learning models be used as surrogate models in simulations.

The effect of meta-modeling was limited to RFs, ERTs, and ANNs but from this we were able to ascertain that there is no adverse effects on ANNs and no effects on RFs and ERTs. Future work would include using meta-modeling on GPs and linear regressors. 



\begin{comment}

\section{Results2}\label{sec:results_2}
The list of initial control parameters is the same for every model is rewritten below: 
\begin{itemize}
	\item $Z_{eff}, I_p, B_T, a, \delta, M_{eff}, P_{NBI}, P_{ICHR}, P_{TOT}, q_{95}, \Gamma, H, P_{SD}$.
\end{itemize}
The correlation matrix and Spearman rank-order correlation plots are found in Figures \ref{fig:corrshit}

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth, keepaspectratio=true]{./src/correlation_shit}
	\caption{LHS: the Spearman rank-order correlation plot.  RHS: the correlation matrix for the initial control parameters}
	\label{fig:corrshit}
\end{figure}
\subsection{Linear Regression}
\begin{figure}[t!p]
	\begin{subfigure}[b]{0.45\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth, keepaspectratio=true]{./src/Lasso_feature_selection}
		\caption{}
		\label{subfig:lasso_FS}
	\end{subfigure}
	\begin{subfigure}[b]{0.5\linewidth}
		\centering 
		\includegraphics[width=0.9\linewidth, keepaspectratio=true]{./src/regressor_using_Lasso_feature_selection}
		\caption{}
		\label{subfig:reg_FS}
	\end{subfigure}
	\caption{Feature selection (a) and prediction quality (b) for linear regression}
	\label{fig:lin_exp}
\end{figure}
I compare how the Lasso method of feature selection works with two different version of the dataset: the 'as is' data, the data scaled such that each feature has a mean of 0 and spread of 1.
It is important to note that the prediction quality of the Lasso models is not entirely important, as $\lambda = 0$ would yield the best results, but would not indicate importance of features at all.  
The results of the LASSO method for various values of $\lambda$ on the two versions of the dataset, non scaled and scaled, are plotted in \ref{subfig:lasso_FS}. 
For the nonscaled data, by averaging over all the differnt values of $\lambda$, the following features were deemed most important: $I_p, \delta, P_{NBI}, P_{ICRH}, V_P, P_{TOT}, q_{95}, \Gamma$.
For the scaled data, the following features were deemed important: $I_p, \delta, P_{NBI}, B_T, q_95, \Gamma, H$. 
Interestingly enough, both the scaled and nonscaled data deemed the following features as not relevant: $B_T, Z_{eff}, M_{eff}, a, P_{SD}$, where $B_T$ seems most suprising to me. 
Then, aggregating new feature spaces using the two sets of generated features, a linear regressor (using OLS and without regularization) is fit and compared against the scaling law in Figure~\ref{subfig:reg_FS}. Both the new sets produce regression predictors better than the scaling law, with the regressor trained on the non-scaled fit performing substaintially better than than both. Both do are able to capture higher values of $n_e^{ped}$ better than the scaling law, but slightly struggle on those close to 0. 
Using features determined from the Lasso method on non-scaled data, it seems viable that a new scaling law could be written using the coefficients of a regressor fitted using those features. 
For UQ of linear regression tools, I employ Baysian regression, which transforms the coefficients to distributions, giving the overall uncertainty in the prediction from the spread of each of the weight distributions. 

\subsection{Gaussian Processes}
\begin{figure}
	\begin{subfigure}{0.5\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth, keepaspectratio=true]{./src/ARD_vs_KL_VAR_feature_relevance_final_maybe}
		\caption{}
		\label{subfig:gpfs}
	\end{subfigure}
	\begin{subfigure}{0.5\linewidth}
		\centering
		\includegraphics[width=1\linewidth, keepaspectratio=true]{./src/GP_kernels_num_features}
		\caption{}
		\label{subfig:gp_dim}
	\end{subfigure}
	\caption{Feature selection (a) and Prediction quality (b) for Gaussian Processes}
	\label{fig:gp_exp}
\end{figure}
Before testing out ARD, KLD, and VAR, both control features and $n_e^{ped}$ are scaled such that they have mean of 0 and spread of 1. This is necessary for GPs. 
The features can be ranked through the use of ARD, KLD, and VAR, and are listed below in order of importance as plotted in \ref{subfig:gpfs}. 
\begin{itemize}
	\item \textbf{ARD}: $\delta, I_p, Z_{eff}, a, \Gamma, q_{95}, B_T, P_{NBI}, P_{ICRH}, V_P, P_{TOT}, H, M_{eff}$
	\item \textbf{KLD}: $\delta, I_p, a, Z_{eff}, P_{NBI}, q_{95}, B_T, \Gamma, P_{TOT}, V_P, P_{ICRH}, H, M_{eff}$
	\item \textbf{VAR}: $\delta, a, I_p, Z_{eff}, B_T, q_{95}, P_{TOT}, V_P, \Gamma, P_{NBI}, P_{ICRH}, H, M_{eff}$
\end{itemize}
$P_{SD}$ is not included, as it seemed to play absolutely no benefit in the gaussian process predictions, and would tend to disrupt the feature selection methods when included. 
Although they may rank each feature differently, it is interesting to note that the top 4 features that are relevant to predicting $n_e^{ped}$ are always triangularity, current, minor radius and effective charge. Furthermore, where the lasso disregarded $Z_{eff}, B_T$ and $a$, the GP methods consitently rank these among in addition to the transport in the plasma of the highest of importance. The reason each method consistantly ranks the power parameters among the lowest is because the dependence of $n_e^{ped}$ on power is relatively weak. Just like in the scaling law, 

Following the selection of features, I fit 14 GP models using the KLD order, with the first model using onlly $\delta$ as input, thus dimensionality of 1, followed by the next model using $\delta, I_p$, with dimensionality 2, and so on following the KLD order of importance. The results are plotted against the RMSE of each fitted model while comparing three different kernels in Figure \ref{subfig:gp_dim}. NOTE.TO.AARO: [This plot is not finished, I need to rerun the experiment using the KLD, VAR, and ARD methods and make three plots to compare. Also the dimensionality list is not actually that of KLD but slightly different. So need to particullary rerun KLD ]. From the plot, I deduce that the Rational Quadratic (RQ) kernel outpreforms the Matern kernel, which in turn outpreforms the Radial Basis Function (RBF). For both RQ and Matern, the RMSE does not improve after dimensionality 10, which happens to be $V_P$, thus further confirming the unimportance of $V_P, P_{ICRH}, H, M_{eff}$. The uncertainty lines atttached to each point are from the cross validation methods used. 

\subsection{Random Forests and Extreme Random Trees}
\begin{figure}
	\begin{subfigure}{0.5\linewidth}
		\centering 
		\includegraphics[width=0.95\linewidth, keepaspectratio=true]{./src/RF_oob_error}
		\caption{}
		\label{subfig:rfoob}
	\end{subfigure}
	\begin{subfigure}{0.5\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, keepaspectratio=true]{./src/RF_RMSE_features_vs_num_estimators}
		\caption{}
		\label{subfig:rf_rmse}
	\end{subfigure}
	\caption{OOB error (a) and prediction quality (b) for Random Forests}
	\label{fig:RF_exp}
\end{figure}

The precise architecture of the RFs and ERTs was determined by grid searching across a domain of parameters (hidden leaf size, number of trees, split impurity size). The best preforming RF was that of \textcolor{green}{HYPEREPARAMS FOR RF} and that of the ERT was\textcolor{green}{HYPERPARAMETERS ERT}.
The results of the OOB error are visualized in Figure \ref{subfig:rfoob}, and the quality of performance in Figure~\ref{subfig:rf_rmse} as a function of the number of decision trees used to build the RF. Both plots compare the amount of features used to build the RF. From the OOB expereiment, we see that the RFs that made use of 6 to 11 features are all converge to relative same levels of OOB errors. These RFs indicate the folowing order of relevance of control parameters, where $x_i / x_j$ means that more often was $x_i$ ranked more important, but sometimes the RF chose $x_j$ ahead of $x_i$:
\begin{itemize}
	\item $I_p, \delta, \Gamma / q_{95}, a / B_T,  V_P / Z_{eff}, P_{NBI} /  P_{TOT}$ followed by very little relevance of $P_{SD}, P_{ICRH}, H, M_{eff}$
\end{itemize}

To see the power parameters ranked so low is quite surprising in comparison to their placement with GP and linear regressor methods of feature selection. 
Just like GPs, the 3 features that were consistently were ranked least important are $H, M_{eff},$ and $P_{ICRH}$.

When using permutation importance via permuting the cluster of features found in \ref{fig:corrshit}, it was found that removing the plasma volume $V_P$ and total power $P_{TOT}$ where the only parameters that could be removed that did end up improving the performance of RF's. The improvement was less than marginal, a decrease in the RMSE and MAE by $0.01$. However, this is an improvement overall, since the dimensionality of the feature space can be reduced for RF's, in addition to the methods of feature selection above, will result in quicker training cycles and faster prediction times overall. 

\begin{figure}
	\begin{subfigure}{0.5\linewidth}
		\centering
		\includegraphics[width=0.99\linewidth, keepaspectratio=true]{./src/RF_predictions_and_uncert}
		\caption{}
		\label{subfig:RF_UQ}
	\end{subfigure}
	\begin{subfigure}{0.5\linewidth}
		\centering
		\includegraphics[width=0.99\linewidth, keepaspectratio=true]{./src/RF_UQ}
		\caption{}
		\label{subfig:RF_UQ2}
	\end{subfigure}
	\caption{}
	\label{fig:RF_Preds}
\end{figure}
In regards to the UQ of Random Forests, since there are $\approx 2000$ entries, attaching little error bars to each point estimate would not show much on a graph, therefore in Figure\ref{subfig:RF_UQ} I plot on the LHS the point predictions and on the RHS the determined uncertainty against the values of true $n_e^{ped}$. It is quite obvious that the uncertainty in prediction follows quite closely the distribution of entries for $n_e^{ped}$ as seen in \ref{subfig:RF_UQ2}, i.e., our uncertainty in an estimate is largely based on the amount of representation that estimate has in the dataset. 
\textcolor{red}{ERTs TBD}

\subsection{ANN's}
\textcolor{red}{TBD}

\subsection{Meta-Modeling}

\subsubsection{RFs and ERTs}
RF's split the leafs in each decision tree as to minimize the MSE, so by nature, the inclusion of synthesized values will not change the way the each tree is made unless it will improve predictions. It was seen that even with including up to 1000 synthesized entries (added during the CV split procedure, so essentially double the training entries were of $n_e^{ped} \geq 10.5$), no change in the performance of the RF was seen. 

\section{Discussion}\label{sec:discussion}
In predicting $n_e^{ped}$, it seems that parameters like $M_{eff}, H, P_{ICHR}$ have little to no relevance.

\end{comment}

\begin{comment}


\subsection{Uneccesary details / Appendix}

\section{Linear Regression}
Under the hood, linear regression uses inputs $\vec{x}$ to make an estimate $\hat{y}$ on what $y$, via the following prediction function:

\begin{equation} \label{eq:linreg}
	\hat{y} = f(\vec{x}) = \vec{w} \vec{x} + \epsilon
\end{equation}

where $\vec{w} = (w_1, w_2, \cdots, w_p)$ is a vector of parameters or coefficients that are multiplied by each feature $x_i$ before summing up the contributions from each feature, and $\epsilon$ is the irreducible error.
It is common to refer to the coefficients as \textit{weights} that determine how each control parameter affects the prediction of $n_e^{ped}$. If the coefficient is positive, then as that feature increases in value, then so will the prediction of $n_e^{ped}$, whereas if the coefficient is negative, then increasing the feature value will lead to a decrease in the prediction value. If the weight is 0, then the feature will play no role in the prediction, whereas if it is very large, it will dominate the effects on the prediction. The weight values are deterimned via ordinary least squares algorithm [SOURCE]. The general idea is to minimize the euclidian distance between the target and predictions using the mean squared error rewritten below.
\begin{equation}\label{eq:OLS}
	MSE_{OLS} = \frac{1}{N} \sum_i \left( y_i -  \sum_{j=1}^p x_{ij} w_j \right)^2
\end{equation}
Thus there exists an optimal set of weights for each of the features that minimize this cost function. It could be that the simple linear terms do not fully capture the relationship between $\vec{x}$ and $y$, and one would like to add polynomial terms to the prediction function \ref{eq:linreg}, and the polynomial terms would have their own coefficients. This is one way to increase a linear model's representational capacity, however at the same time increases the model complexity, and could potentially lead to overfitting. It is preferred to reduce model complexity and overfitting as much as possible without sacrificing performance. To reduce complexity without sacrificing performance, regularization is used. Regularization can be thought of as any modification one makes to a learning algorithm that is intended to reduce its generalization error, but not its training erro.  This can be done in the case of linear regression by adding a penalty term to the cost function; one such method, $L^1$ regualrization, or \textit{weight decay}, is preformed by comprimising both the MSE written above, and a the criterion $\mathbb{L}$ that expresses the preference for having smaller weights than the $L^1$ norm.
\begin{equation}
	\mathbb{L}(\vec{w})_{L1} = MSE_{OLS} +  \lambda |\vec{w}|;
\end{equation}
where $\lambda \in \mathbb{R}$ represents the regularization term and is chosen prior to the fitting procedure. For $\lambda =0$, the OLS cost function does not change.
Minimizing $\mathbb{L}(\vec{w})$ results in a choice of weights that balance well fitting of the data while keeping the weights as small as possible.
A regression model that minimizes this particular cost function is called a Lasso model. The neat trick with this form of regularization, is that it is possible for the weights of a certain features to reduce to near zero in order to achieve the lowest MSE. Weight coeficcients of control parameters that are contribute "less" to the regression function drop to zero, effectively acting as a form of \textit{feature selection}[SOURCE], deeming which control parameters are important for the linear regressor and its task of predicting $n_e^{ped}$.
In Section 3, I show which control imputs are deemed important by the Lasso procedure, as well as compare the scaling law to a linear regressor that uses the features determined by the Lasso as inputs.

\subsubsection{Baysian Linear Regression for UQ}
One of the downsides of linear regression, and most machine learning models, is that they produce point predictions of a quantity. In science, we are not only interested in a prediction, but also how uncertain we are in the prediction. For linear regressors, the point prediction can vary depending on the weights, thus any change to the weights will result in a change in the prediction. To quantify how certain we are with the choice of weights of a regressor is to quantify the uncertainty in its prediction. If each weight $w_i$ in $\vec{w}$ in \ref{eq:linreg} is a scalar quantity, this is quite difficult. However, if the weights were instead normal distributions, with the mean centered around the desired coefficient, then the uncertainty would be the spread of the weight's distribution. This is called Bayseian linear regression, which operates much like the above process.

\subsection{Gaussian Processes}
Instead of searching the space of possible coefficients that best parameterize the prediction function \ref{eq:linreg}, what if we lookd for the best prediction function to parameterize $n_e^{ped}$? This is the idea behind Gaussian Processes(GPs). For example, fitting a quadratic function using equation \ref{eq:linreg} would be futile, so by adding further polynomial terms one should converge to a more viable solution. But in the case of finding a prediction function for $n_e^{ped}$, we do not know what kind of function will give the best fit. The GP approach is thus a \textit{non-parametric} approach, and finds a distribution over the possible functions $f(x)$. Now there are a lot of functions that could fit $n_e^{ped}$, so by scaling $n_e^{ped}$ to have a mean of 0 and spread of 1, the domain of sample functions are reduced to those that produce outputs with similar mean and spread. Some of these functions may be very 'wiggly', i.e., they bounce from point to point quite quickly, whereas a smoother function ( not so 'wiggly'), is that which resembles splicing techniques. To specify the smoothness, a covariance matrix, or \textit{kernel} is used, such that values that are close together in input space will produce output values that are close together.  Lots of math goes into this, which can be found in the following [SOURCES]. Within the context of this thesis, I am interested in determining which kernel produces an optimal distribution of functions. In section 3, I show which kernels give the best performance score on predicting $n_e^{ped}$.

Just like with Lasso linear regression, the relevance of input variables of a fitted GP model can be determined, but intsead of the magnitude of the scaler weight coeficcient, it is instead the length-scale parameters of the kernel used (DEFINE LENGTH-SCALE BEFORE)[SOURCE]. In the context of bayesian modeling, this is called automatic relevance detmination [SOURCE]. Alternatively, feature selection for GP's can be done using sensitivty analysis, with detailed explaination of the math behind sensitivy analysis found in [SOURCE]. There are two types of sensitivity analysis used in this thesis, the first is through the use of  the Kullback-Leibler Divergence (KLD) [SOURCE]. The difference in KLD when an input $\vec{x}$ is altered with respect to a single feature $x_i$ will tell the predictive relevance; a large difference in the KLD indicates that the feature has high predictive relecance, and with low difference in KLD the feature has low predictive relevance. The second type of sensitivity analysis used in this thesis is teh Variance of the Posterior (VAR). VAR makes uses of the latent mean of a GP, and specifically looks at the variability of this latent mean when the value of a single feature is changed; large variability in the latent mean indicates that the feature is relevant in predicting $n_e^{ped}$, and low variability in the latent mean indicates it is not.

In section 3, I show which features are selected by GP's using ARD, KLD, and VAR for various kernels.

\subsubsection{UQ for GPs}
Fitting a GP yields a joint-gaussian group of functions, which when given an input of control parameters, each function within the group gives a prediction of $n_e^{ped}$, and the mean of all of these outputs is then the final point prediction of $n_e^{ped}$. The UQ is built into the GP in that the uncertainty in the mean point preidction is just the standard deviation of the outputs across all of the functions used in the mean. With GPs, not only is it possible to quantify the uncertainty in the prediction, but also to make use of the uncertainties in the data. To do this, \textcolor{blue}{need more detail about how this is possible}.

\subsection{Random Forests}
The intuition for random forests (RFs) is very different from that of the above mentioned regression techniques. For detailed information on what decision trees are and how they work well as when combined as RFs regressors, see [SOURCES]. A main difference in traditional regression tools and random forests is the way in which data is sampled to be fed to the regressor. RFs make use of the method of \textit{bootstrap aggregation}, in which the data is randomly sampled from the larger dataset to create the trees within the random forest. The method is a bit more complicated than just randomly sampling, but more can be read here [SOURCE]. In the context of this thesis, it is important to know that each tree in the RF uses a different sampled subset of data to manifest itself, i.e., not every tree is built using the same subset of data, and some entries will be contained in the fitting of the tree and some not.
Furthermore, in addition to randomly sampling the data points to create random subsets of data, RFs also sample random susbets of the features. The number of features to randomly select from the available control parameters set is chosen before fitting and is thus a hyperparameter. The features then chosen for each tree can be compared, and the relevance of the features is then quanitified from the number of trees that chose them vs how many trees in total there are in the forest \textcolor{blue}{describe how the relevance is chosen from the trees dropping out if they are shit}. In order to find the optimal number of features to use, I study the \textit{out-of-bag} (OOB) error. The OOB error is the average error for each prediction of a sample from the training set that is \textbf{not} contained in a tree[SOURCE]. The OOB error indicates how well a forest extrapolates the information, and a low OOB error refers to the RF that can best generalize predicting $n_e^{ped}$.
In Section 3, I compare the OOB error for different number of features used to determine the optimal number for RF, as well as discuss which features are selected in this process.

In addition to the out-of-bag error as an approach to feature selection, permutation importance (PI) can also be used. PI works particularly well for non-linear models like RFs and ERTs, and is defined as the decrease inamodel score when asingle feature value is randomly shuffled (corrupted). Similar to KLD, where a singular data entry is altered feature wise, PI instead shuffles the value of entire features and checks to see how the model reacts [SOURCE]. Permutating the feature space in this way yeilds the importance of a feature that is the models score when fitted on the shuffled dataset subtracted form the baseline score of the model fitten on the unperterbed case. However, when two features are strongly correlated, in the case of the power trio $P_{NBI}, P_{ICRH}, P_{TOT}$, the model will still have access to the feature via proxy with the correlated feature. To handle this, I employ a clustering technique in cluster correlated features based on their Spearman rank-order correelations, and then keep only one feature form each cluster. In Section 3 I show the results of the Spearman rank-order correlations by showing which parameters, when taken out from the list of input features, improve prediction quality.

\subsubsection{Extreme Random Trees}
All of the above semantics can be applied to Extreme Random Trees(ERTs). ERTs are slightly different from RF's, in that \textcolor{blue}{Describe ERTs and how they are different, advantages and disadvantages}

\subsubsection{UQ}
Within a RF there are $n$ decision trees built, and in order to quanitfify the uncertainty of the RF, I take the standard deviation of the prediction of each decision tree making up the RF. Additionally, one can use the ensemble method similar to that which is explained for ANN's later, in which the uncertainty in the ensemble becomes the uncertainty in the prediction, even further could one take the standard deviation of all the decision trees in the ensemble of random forests used.

\subsection{ANN's}
In this thesis, only feed forward networks are used for the prediction of $n_e^{ped}$.
The use feed-forward neural networks as a regresssion teqnique is no new task, and using a series of nested non-linear function to provide continous predictions on $n_e^{ped}$ makes this regression solution different only in method. T
\textcolor{blue}{TBD. }

\subsection{Meta-Modeling}
Another use of the uncertainties of each parameter is to generate synthesized values by using the measured parameter value as the mean of normal distribution and its uncertainty as the spread.
By sampling from the distribution of each parameter, it is possible to generate new 'synthesized' values. As seen in the next section, many models can predict well for $n_e^{ped} \leq 10$, but struggle for densities higher than that. Since ML the models above are very dependent on the data used to fit them, and consdiering that in the dataset there are less than 10 shots that have $n_e^{ped} \geq 10$, by including synthesized values in the fitting procedure the hope was that the models would be able to predict bettter on held out set that did not include the synthesized values.

\end{comment}
\bibliographystyle{unsrt}
\bibliography{biblioski}
\end{document}
